{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a16be32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:12.215204Z",
     "iopub.status.busy": "2025-03-26T11:49:12.214901Z",
     "iopub.status.idle": "2025-03-26T11:49:16.504106Z",
     "shell.execute_reply": "2025-03-26T11:49:16.503052Z"
    },
    "papermill": {
     "duration": 4.296968,
     "end_time": "2025-03-26T11:49:16.505749",
     "exception": false,
     "start_time": "2025-03-26T11:49:12.208781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, pathlib, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b238e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:16.516184Z",
     "iopub.status.busy": "2025-03-26T11:49:16.515840Z",
     "iopub.status.idle": "2025-03-26T11:49:16.581306Z",
     "shell.execute_reply": "2025-03-26T11:49:16.580433Z"
    },
    "papermill": {
     "duration": 0.071736,
     "end_time": "2025-03-26T11:49:16.582628",
     "exception": false,
     "start_time": "2025-03-26T11:49:16.510892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9412ba04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:16.593074Z",
     "iopub.status.busy": "2025-03-26T11:49:16.592847Z",
     "iopub.status.idle": "2025-03-26T11:49:16.595866Z",
     "shell.execute_reply": "2025-03-26T11:49:16.595271Z"
    },
    "papermill": {
     "duration": 0.009576,
     "end_time": "2025-03-26T11:49:16.597150",
     "exception": false,
     "start_time": "2025-03-26T11:49:16.587574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_nodes = 2\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c98e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:16.607032Z",
     "iopub.status.busy": "2025-03-26T11:49:16.606761Z",
     "iopub.status.idle": "2025-03-26T11:49:25.380581Z",
     "shell.execute_reply": "2025-03-26T11:49:25.379436Z"
    },
    "papermill": {
     "duration": 8.780283,
     "end_time": "2025-03-26T11:49:25.382114",
     "exception": false,
     "start_time": "2025-03-26T11:49:16.601831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 195996\n",
      "Validation samples: 42004\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths for both datasets\n",
    "train_data_paths = [\n",
    "    r\"/kaggle/input/jpd-df2-lfcc/LFCC_T1/train\",  # Language 1\n",
    "    r\"/kaggle/input/jpd-df2-lfcc-t2/LFCC/train\"   # Language 2\n",
    "]\n",
    "validation_data_paths = [\n",
    "    r\"/kaggle/input/jpd-df2-lfcc/LFCC_T1/val\",\n",
    "    r\"/kaggle/input/jpd-df2-lfcc-t2/LFCC/val\"\n",
    "]\n",
    "\n",
    "class MixedPtDataset(Dataset):\n",
    "    def __init__(self, directories):\n",
    "        \"\"\"Load features from multiple directories.\"\"\"\n",
    "        self.files = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        for directory in directories:\n",
    "            classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "            \n",
    "            # Assign class indices if not already assigned\n",
    "            for c in classes:\n",
    "                if c not in self.class_to_idx:\n",
    "                    self.class_to_idx[c] = len(self.class_to_idx)\n",
    "\n",
    "            for c in classes:\n",
    "                c_dir = os.path.join(directory, c)\n",
    "                c_files = [(os.path.join(c_dir, f), self.class_to_idx[c]) for f in os.listdir(c_dir)]\n",
    "                self.files.extend(c_files)\n",
    "\n",
    "        random.shuffle(self.files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, label = self.files[idx]\n",
    "        try:\n",
    "            mat_vals = scipy.io.loadmat(filepath)\n",
    "            data = mat_vals['final'].T\n",
    "            max_len = 800\n",
    "            if max_len > data.shape[0]:\n",
    "                pad_width = max_len - data.shape[0]\n",
    "                data = np.pad(data, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                data = data[:max_len, :]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {filepath}: {str(e)}\")\n",
    "            return None\n",
    "        return data, label\n",
    "\n",
    "# Combine both datasets\n",
    "train_dataset = MixedPtDataset(train_data_paths)\n",
    "val_dataset = MixedPtDataset(validation_data_paths)\n",
    "\n",
    "class PtDataLoader(DataLoader):\n",
    "    def __init__(self, directories, batch_size, shuffle=True):\n",
    "        dataset = MixedPtDataset(directories)\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Load mixed datasets\n",
    "batch_size = 32\n",
    "train_dataloader = PtDataLoader(directories=train_data_paths, batch_size=batch_size)\n",
    "val_dataloader = PtDataLoader(directories=validation_data_paths, batch_size=batch_size)\n",
    "\n",
    "train_count = len(train_dataset)\n",
    "val_count = len(val_dataset)\n",
    "\n",
    "print(f\"Training samples: {train_count}\\nValidation samples: {val_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d829f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:25.392798Z",
     "iopub.status.busy": "2025-03-26T11:49:25.392490Z",
     "iopub.status.idle": "2025-03-26T11:49:25.396363Z",
     "shell.execute_reply": "2025-03-26T11:49:25.395467Z"
    },
    "papermill": {
     "duration": 0.010581,
     "end_time": "2025-03-26T11:49:25.397668",
     "exception": false,
     "start_time": "2025-03-26T11:49:25.387087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195996\n",
      "42004\n"
     ]
    }
   ],
   "source": [
    "print(train_count)\n",
    "# print(test_count)\n",
    "print(val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ac7081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:25.407830Z",
     "iopub.status.busy": "2025-03-26T11:49:25.407569Z",
     "iopub.status.idle": "2025-03-26T11:49:25.410824Z",
     "shell.execute_reply": "2025-03-26T11:49:25.410200Z"
    },
    "papermill": {
     "duration": 0.009381,
     "end_time": "2025-03-26T11:49:25.411994",
     "exception": false,
     "start_time": "2025-03-26T11:49:25.402613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326d7e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:25.421859Z",
     "iopub.status.busy": "2025-03-26T11:49:25.421606Z",
     "iopub.status.idle": "2025-03-26T11:49:25.425265Z",
     "shell.execute_reply": "2025-03-26T11:49:25.424319Z"
    },
    "papermill": {
     "duration": 0.010234,
     "end_time": "2025-03-26T11:49:25.426678",
     "exception": false,
     "start_time": "2025-03-26T11:49:25.416444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "input_size = 20\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "# drop_amount = 0.25  # You can choose an appropriate dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7a86d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:25.437129Z",
     "iopub.status.busy": "2025-03-26T11:49:25.436913Z",
     "iopub.status.idle": "2025-03-26T11:49:25.440208Z",
     "shell.execute_reply": "2025-03-26T11:49:25.439340Z"
    },
    "papermill": {
     "duration": 0.009629,
     "end_time": "2025-03-26T11:49:25.441345",
     "exception": false,
     "start_time": "2025-03-26T11:49:25.431716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98df71a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:25.451440Z",
     "iopub.status.busy": "2025-03-26T11:49:25.451185Z",
     "iopub.status.idle": "2025-03-26T11:49:45.242138Z",
     "shell.execute_reply": "2025-03-26T11:49:45.241304Z"
    },
    "papermill": {
     "duration": 19.798101,
     "end_time": "2025-03-26T11:49:45.243982",
     "exception": false,
     "start_time": "2025-03-26T11:49:25.445881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, pathlib, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from transformers.models.whisper.modeling_whisper import WhisperModel, WhisperEncoder\n",
    "from transformers.models.whisper.configuration_whisper import WhisperConfig\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "import librosa \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob, pickle\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0260b4a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.255209Z",
     "iopub.status.busy": "2025-03-26T11:49:45.254699Z",
     "iopub.status.idle": "2025-03-26T11:49:45.258266Z",
     "shell.execute_reply": "2025-03-26T11:49:45.257596Z"
    },
    "papermill": {
     "duration": 0.010265,
     "end_time": "2025-03-26T11:49:45.259477",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.249212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddc3e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.270572Z",
     "iopub.status.busy": "2025-03-26T11:49:45.270294Z",
     "iopub.status.idle": "2025-03-26T11:49:45.276612Z",
     "shell.execute_reply": "2025-03-26T11:49:45.275768Z"
    },
    "papermill": {
     "duration": 0.013097,
     "end_time": "2025-03-26T11:49:45.277910",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.264813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_amount = 0.255\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=drop_amount)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device=x.device, dtype=torch.double)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device=x.device, dtype=torch.double)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        # Extract the output of the last time step from both directions\n",
    "        last_hidden_state = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)\n",
    "        output = self.fc(last_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba94a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.288884Z",
     "iopub.status.busy": "2025-03-26T11:49:45.288599Z",
     "iopub.status.idle": "2025-03-26T11:49:45.292100Z",
     "shell.execute_reply": "2025-03-26T11:49:45.291242Z"
    },
    "papermill": {
     "duration": 0.010381,
     "end_time": "2025-03-26T11:49:45.293337",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.282956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_nodes = 2\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e5f554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.304114Z",
     "iopub.status.busy": "2025-03-26T11:49:45.303862Z",
     "iopub.status.idle": "2025-03-26T11:49:45.664290Z",
     "shell.execute_reply": "2025-03-26T11:49:45.663369Z"
    },
    "papermill": {
     "duration": 0.367307,
     "end_time": "2025-03-26T11:49:45.665678",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.298371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (lstm): LSTM(20, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.255, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device, dtype=torch.double)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4bf393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.676152Z",
     "iopub.status.busy": "2025-03-26T11:49:45.675930Z",
     "iopub.status.idle": "2025-03-26T11:49:45.682082Z",
     "shell.execute_reply": "2025-03-26T11:49:45.681354Z"
    },
    "papermill": {
     "duration": 0.012633,
     "end_time": "2025-03-26T11:49:45.683266",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.670633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_amount = 0.255\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout = nn.Dropout(p=drop_amount)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming input x shape is (batch_size, sequence_length, input_channels)\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, input_channels, sequence_length)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.adaptive_max_pool1d(x, 1)  # Pool to a fixed size (1)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a57da51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.699402Z",
     "iopub.status.busy": "2025-03-26T11:49:45.699063Z",
     "iopub.status.idle": "2025-03-26T11:49:45.703314Z",
     "shell.execute_reply": "2025-03-26T11:49:45.702400Z"
    },
    "papermill": {
     "duration": 0.013947,
     "end_time": "2025-03-26T11:49:45.704944",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.690997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_nodes = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "input_channels = 20  # You can change this depending on your input data\n",
    "num_classes = 2  # Adjust according to your classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5019754b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.716154Z",
     "iopub.status.busy": "2025-03-26T11:49:45.715860Z",
     "iopub.status.idle": "2025-03-26T11:49:45.728011Z",
     "shell.execute_reply": "2025-03-26T11:49:45.726711Z"
    },
    "papermill": {
     "duration": 0.019596,
     "end_time": "2025-03-26T11:49:45.729466",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.709870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv1): Conv1d(20, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.255, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier(input_channels, num_classes)\n",
    "model.to(device, dtype=torch.double)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f16041f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.742331Z",
     "iopub.status.busy": "2025-03-26T11:49:45.742074Z",
     "iopub.status.idle": "2025-03-26T11:49:45.748363Z",
     "shell.execute_reply": "2025-03-26T11:49:45.747625Z"
    },
    "papermill": {
     "duration": 0.014063,
     "end_time": "2025-03-26T11:49:45.749718",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.735655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_amount = 0.255\n",
    "\n",
    "class BiGRUAudioClassifier(nn.Module):\n",
    "    def __init__(self,input_size, num_classes, hidden_units, num_layers):\n",
    "        super(BiGRUAudioClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.bigru = nn.GRU(input_size=input_size, hidden_size=hidden_units, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=drop_amount)\n",
    "        # self.fc = nn.Linear(hidden_units, num_classes)\n",
    "        self.fc = nn.Linear(hidden_units * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, num_features)\n",
    "\n",
    "        # Pass the input through the bi-GRU layers\n",
    "        output, _ = self.bigru(x)\n",
    "        output = self.dropout(output)\n",
    "        # Extract the last hidden state (concatenate forward and backward hidden states)\n",
    "        last_hidden_state = torch.cat((output[:, -1, :self.hidden_units], output[:, 0, self.hidden_units:]), dim=1)\n",
    "        # Apply the fully connected layer for classification\n",
    "        output = self.fc(last_hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8b8e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.764447Z",
     "iopub.status.busy": "2025-03-26T11:49:45.764121Z",
     "iopub.status.idle": "2025-03-26T11:49:45.790025Z",
     "shell.execute_reply": "2025-03-26T11:49:45.789015Z"
    },
    "papermill": {
     "duration": 0.034906,
     "end_time": "2025-03-26T11:49:45.791488",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.756582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiGRUAudioClassifier(\n",
      "  (bigru): GRU(20, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.255, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 20\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "model = BiGRUAudioClassifier(input_size, num_classes, hidden_size, num_layers)\n",
    "model.to(device, dtype=torch.double)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71a5037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.802842Z",
     "iopub.status.busy": "2025-03-26T11:49:45.802572Z",
     "iopub.status.idle": "2025-03-26T11:49:45.815215Z",
     "shell.execute_reply": "2025-03-26T11:49:45.814409Z"
    },
    "papermill": {
     "duration": 0.01992,
     "end_time": "2025-03-26T11:49:45.816634",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.796714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Basic Block for ResNet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define the ResNet architecture\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Function to create ResNet-50 model\n",
    "def resnet50(num_classes=2):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae85e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:45.827279Z",
     "iopub.status.busy": "2025-03-26T11:49:45.827068Z",
     "iopub.status.idle": "2025-03-26T11:49:46.061290Z",
     "shell.execute_reply": "2025-03-26T11:49:46.060264Z"
    },
    "papermill": {
     "duration": 0.24109,
     "end_time": "2025-03-26T11:49:46.062890",
     "exception": false,
     "start_time": "2025-03-26T11:49:45.821800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50()\n",
    "num_epochs=10\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fcbe74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T11:49:46.074714Z",
     "iopub.status.busy": "2025-03-26T11:49:46.074437Z",
     "iopub.status.idle": "2025-03-26T15:07:58.904497Z",
     "shell.execute_reply": "2025-03-26T15:07:58.903561Z"
    },
    "papermill": {
     "duration": 11892.837528,
     "end_time": "2025-03-26T15:07:58.906044",
     "exception": false,
     "start_time": "2025-03-26T11:49:46.068516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [38:13<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [07:25<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10   Train Loss: 0.6946   Train Accuracy: 0.5022   Validation Accuracy: 0.4991\n",
      "\n",
      "Epoch 2/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [15:16<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:16<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10   Train Loss: 0.6947   Train Accuracy: 0.5008   Validation Accuracy: 0.4999\n",
      "\n",
      "Epoch 3/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:18<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:03<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10   Train Loss: 0.6947   Train Accuracy: 0.5005   Validation Accuracy: 0.4999\n",
      "\n",
      "Epoch 4/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:11<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:06<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10   Train Loss: 0.6946   Train Accuracy: 0.5013   Validation Accuracy: 0.5028\n",
      "\n",
      "Epoch 5/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:22<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:08<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10   Train Loss: 0.6946   Train Accuracy: 0.5020   Validation Accuracy: 0.4995\n",
      "\n",
      "Epoch 6/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:50<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:14<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10   Train Loss: 0.6947   Train Accuracy: 0.5012   Validation Accuracy: 0.4954\n",
      "\n",
      "Epoch 7/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:59<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:15<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10   Train Loss: 0.6946   Train Accuracy: 0.5013   Validation Accuracy: 0.5036\n",
      "\n",
      "Epoch 8/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [15:03<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:11<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10   Train Loss: 0.6946   Train Accuracy: 0.5011   Validation Accuracy: 0.5029\n",
      "\n",
      "Epoch 9/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [14:37<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:14<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10   Train Loss: 0.6947   Train Accuracy: 0.5017   Validation Accuracy: 0.4993\n",
      "\n",
      "Epoch 10/10 - Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6125/6125 [15:08<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [02:13<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10   Train Loss: 0.6945   Train Accuracy: 0.5019   Validation Accuracy: 0.4993\n",
      "\n",
      "Finished Training\n",
      "Maximum Validation Accuracy: 0.5036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize lists and variables\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "valid_accuracy_list = []\n",
    "\n",
    "num_epochs = 10\n",
    "max_acc = 0\n",
    "pred_labels = []\n",
    "act_labels = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # ðŸ› ï¸ Training Phase\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs} - Training...\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_dataloader, desc=\"Training Batches\")):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reshape and convert tensors to float\n",
    "        images = images.unsqueeze(1).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        train_loss += loss.cpu().data * images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    # Average loss and accuracy over the training set\n",
    "    train_accuracy /= train_count\n",
    "    train_loss /= train_count\n",
    "    \n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # ðŸ› ï¸ Validation Phase\n",
    "    model.eval()\n",
    "    valid_accuracy = 0.0\n",
    "    pred = []\n",
    "    lab = []\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation...\")\n",
    "\n",
    "    # Use torch.no_grad() for faster inference during validation\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_dataloader, desc=\"Validation Batches\")):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Reshape and convert tensors to float\n",
    "            images = images.unsqueeze(1).float()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            \n",
    "            valid_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "            pred.extend(prediction.tolist())\n",
    "            lab.extend(labels.tolist())\n",
    "\n",
    "    # Average validation accuracy\n",
    "    valid_accuracy /= val_count\n",
    "    valid_accuracy_list.append(valid_accuracy)\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_accuracy > max_acc:\n",
    "        pred_labels = pred\n",
    "        act_labels = lab\n",
    "        max_acc = valid_accuracy\n",
    "        torch.save(model, 'best_model.pth')\n",
    "    \n",
    "    # Display epoch summary\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}   \"\n",
    "          f\"Train Loss: {train_loss:.4f}   \"\n",
    "          f\"Train Accuracy: {train_accuracy:.4f}   \"\n",
    "          f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "# âœ… Final results\n",
    "print(\"\\nFinished Training\")\n",
    "print(f\"Maximum Validation Accuracy: {max_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa62119c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T15:08:06.195892Z",
     "iopub.status.busy": "2025-03-26T15:08:06.195541Z",
     "iopub.status.idle": "2025-03-26T15:08:06.199288Z",
     "shell.execute_reply": "2025-03-26T15:08:06.198584Z"
    },
    "papermill": {
     "duration": 3.655406,
     "end_time": "2025-03-26T15:08:06.200596",
     "exception": false,
     "start_time": "2025-03-26T15:08:02.545190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# best_model = torch.load('model.pth')\n",
    "\n",
    "# # Put the best_model in evaluation mode\n",
    "# best_model.eval()\n",
    "\n",
    "# # Initialize variables to store results\n",
    "# test_accuracy = 0.0\n",
    "# pred_labels = []\n",
    "# act_labels = []\n",
    "\n",
    "# # Pass validation data through the best model\n",
    "# for i, (images, labels) in enumerate(test_dataloader):\n",
    "#     if torch.cuda.is_available():\n",
    "#         images = Variable(images.cuda())\n",
    "#         labels = Variable(labels.cuda())\n",
    "   \n",
    "#     images = images.unsqueeze(1)\n",
    "#     images = images.float()\n",
    "#     outputs = best_model(images)\n",
    "#     _, prediction = torch.max(outputs.data, 1)\n",
    "   \n",
    "#     test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "   \n",
    "#     pred_labels.extend(prediction.tolist())\n",
    "#     act_labels.extend(labels.tolist())\n",
    "\n",
    "# # Calculate testing accuracy\n",
    "# test_accuracy = test_accuracy / test_count\n",
    "\n",
    "# # Print the testing accuracy\n",
    "# print(\"testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b853fb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T15:08:13.648908Z",
     "iopub.status.busy": "2025-03-26T15:08:13.648592Z",
     "iopub.status.idle": "2025-03-26T15:08:13.652291Z",
     "shell.execute_reply": "2025-03-26T15:08:13.651349Z"
    },
    "papermill": {
     "duration": 3.735121,
     "end_time": "2025-03-26T15:08:13.653947",
     "exception": false,
     "start_time": "2025-03-26T15:08:09.918826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the confusion matrix\n",
    "# import seaborn as sns\n",
    "# conf_mat = confusion_matrix(act_labels, pred_labels)\n",
    "# # Plot confusion matrix heat map\n",
    "# sns.heatmap(conf_mat, cmap=\"flare\",annot=True, fmt = \"g\", \n",
    "#             cbar_kws={\"label\":\"color bar\"},\n",
    "#             xticklabels=train_dataset.classes,\n",
    "#             yticklabels=train_dataset.classes)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.savefig(\"ConfusionMatrix_BiLSTM.png\")\n",
    "# plt.show()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1_score = f1_score(pred_labels, act_labels, average='macro')\n",
    "# print('F1 Score : ', f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "179a69a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T15:08:21.105950Z",
     "iopub.status.busy": "2025-03-26T15:08:21.105574Z",
     "iopub.status.idle": "2025-03-26T15:08:21.109792Z",
     "shell.execute_reply": "2025-03-26T15:08:21.108914Z"
    },
    "papermill": {
     "duration": 3.819065,
     "end_time": "2025-03-26T15:08:21.111088",
     "exception": false,
     "start_time": "2025-03-26T15:08:17.292023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import sklearn.metrics\n",
    "\n",
    "# \"\"\"\n",
    "# Python compute equal error rate (eer)\n",
    "# ONLY tested on binary classification\n",
    "\n",
    "# :param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    "# :param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    "# :param positive_label: the class that is viewed as positive class when computing EER\n",
    "# :return: equal error rate (EER)\n",
    "# \"\"\"\n",
    "# def compute_eer(label, pred):\n",
    "#     # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "#     fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n",
    "#     fnr = 1 - tpr\n",
    "\n",
    "#     # the threshold of fnr == fpr\n",
    "#     eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "#     # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "#     eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "#     eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "#     # return the mean of eer from fpr and from fnr\n",
    "#     eer = (eer_1 + eer_2) / 2\n",
    "#     return eer\n",
    "\n",
    "# eer = compute_eer(act_labels, pred_labels)\n",
    "# print('The equal error rate is {:.3f}'.format(eer))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6960546,
     "sourceId": 11156044,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6961676,
     "sourceId": 11157532,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11958.983036,
   "end_time": "2025-03-26T15:08:28.531845",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-26T11:49:09.548809",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
