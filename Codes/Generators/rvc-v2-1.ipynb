{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afce74c3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:33.826118Z",
     "iopub.status.busy": "2025-06-25T01:09:33.825647Z",
     "iopub.status.idle": "2025-06-25T01:09:34.962777Z",
     "shell.execute_reply": "2025-06-25T01:09:34.961544Z"
    },
    "papermill": {
     "duration": 1.155165,
     "end_time": "2025-06-25T01:09:34.964817",
     "exception": false,
     "start_time": "2025-06-25T01:09:33.809652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Retrieval-based-Voice-Conversion'...\r\n",
      "remote: Enumerating objects: 692, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (71/71), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\r\n",
      "remote: Total 692 (delta 39), reused 35 (delta 35), pack-reused 621 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (692/692), 269.29 KiB | 9.62 MiB/s, done.\r\n",
      "Resolving deltas: 100% (312/312), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3ed4ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:34.993547Z",
     "iopub.status.busy": "2025-06-25T01:09:34.993256Z",
     "iopub.status.idle": "2025-06-25T01:09:35.002638Z",
     "shell.execute_reply": "2025-06-25T01:09:35.000919Z"
    },
    "papermill": {
     "duration": 0.02639,
     "end_time": "2025-06-25T01:09:35.004597",
     "exception": false,
     "start_time": "2025-06-25T01:09:34.978207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Retrieval-based-Voice-Conversion\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Retrieval-based-Voice-Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b2a109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:35.032162Z",
     "iopub.status.busy": "2025-06-25T01:09:35.031803Z",
     "iopub.status.idle": "2025-06-25T01:09:36.240575Z",
     "shell.execute_reply": "2025-06-25T01:09:36.239202Z"
    },
    "papermill": {
     "duration": 1.224008,
     "end_time": "2025-06-25T01:09:36.242338",
     "exception": false,
     "start_time": "2025-06-25T01:09:35.018330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-25 01:09:35--  https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/D48k.pth\r\n",
      "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.49, 18.239.50.103, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/db01094a93c09868a278e03dafe8bb781bfcc1a5ba8df168c948bf9168c84d82?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27D48k.pth%3B+filename%3D%22D48k.pth%22%3B&Expires=1750816658&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNjY1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyL2RiMDEwOTRhOTNjMDk4NjhhMjc4ZTAzZGFmZThiYjc4MWJmY2MxYTViYThkZjE2OGM5NDhiZjkxNjhjODRkODI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DrwiNSkZGAQBPqNlU%7EpuxgzzsFgMUi6LbGvNNBRMNHV4zlazekxT%7ESSJynyzwzy-menibrC0iZiG9tBtwb5d1Zz1%7ERXj3cEbcPSyfmZ7NEqv-CxXO033YrcDfFR22s%7EL6Bek6dJK9Wfmjob7zEWhqpIjwFdpWIykqjbkz3dg47LvPJodkUAXD5mkrN7RXUANcD31QwWHAaOT29PEcUG5Y9CalXIS1hM0HA3RHEieBFl1wyk7I%7E0grQaMOTaqZvGmNphj5cVhv0XOE7qJCe702bvEutuVCNFEeaN6JFGWJMhFlTV4H2H%7E671Cgzg3N6mW6Q4zt697BYafXMlmeJ%7EFAA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\r\n",
      "--2025-06-25 01:09:35--  https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/db01094a93c09868a278e03dafe8bb781bfcc1a5ba8df168c948bf9168c84d82?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27D48k.pth%3B+filename%3D%22D48k.pth%22%3B&Expires=1750816658&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNjY1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyL2RiMDEwOTRhOTNjMDk4NjhhMjc4ZTAzZGFmZThiYjc4MWJmY2MxYTViYThkZjE2OGM5NDhiZjkxNjhjODRkODI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DrwiNSkZGAQBPqNlU%7EpuxgzzsFgMUi6LbGvNNBRMNHV4zlazekxT%7ESSJynyzwzy-menibrC0iZiG9tBtwb5d1Zz1%7ERXj3cEbcPSyfmZ7NEqv-CxXO033YrcDfFR22s%7EL6Bek6dJK9Wfmjob7zEWhqpIjwFdpWIykqjbkz3dg47LvPJodkUAXD5mkrN7RXUANcD31QwWHAaOT29PEcUG5Y9CalXIS1hM0HA3RHEieBFl1wyk7I%7E0grQaMOTaqZvGmNphj5cVhv0XOE7qJCe702bvEutuVCNFEeaN6JFGWJMhFlTV4H2H%7E671Cgzg3N6mW6Q4zt697BYafXMlmeJ%7EFAA__&Key-Pair-Id=K3RPWS32NSSJCE\r\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.239.83.30, 18.239.83.10, 18.239.83.87, ...\r\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.239.83.30|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 142875703 (136M) [binary/octet-stream]\r\n",
      "Saving to: ‘pretrained_v2/D48k.pth’\r\n",
      "\r\n",
      "D48k.pth            100%[===================>] 136.26M   358MB/s    in 0.4s    \r\n",
      "\r\n",
      "2025-06-25 01:09:35 (358 MB/s) - ‘pretrained_v2/D48k.pth’ saved [142875703/142875703]\r\n",
      "\r\n",
      "--2025-06-25 01:09:35--  https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/G48k.pth\r\n",
      "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.16, 18.239.50.80, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/2e2b1581a436d07a76b10b9d38765f64aa02836dc65c7dee1ce4140c11ea158b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27G48k.pth%3B+filename%3D%22G48k.pth%22%3B&Expires=1750817375&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNzM3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyLzJlMmIxNTgxYTQzNmQwN2E3NmIxMGI5ZDM4NzY1ZjY0YWEwMjgzNmRjNjVjN2RlZTFjZTQxNDBjMTFlYTE1OGI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AiuJP72hEbIA6bzeeyqVvRuAf-jWkAZUKnkeWacJzBDSpknQKQSI4OYIkIyjd3xjSXepqDNksmw4a7W1QtUmIBiTeBbKMmZuPlFbt48J7i60K-dQryEss0DEsDj5LJCBywD1epxUvD6XTx8cFl0Zgwj8S%7Ed60V%7ERZlIKSywTXs0jwPoELQs0XXdSw2cHT8%7EQJ2GYUsNPV5w1HjteDo%7EjnpoZtLwV9Qxa3MuyMClmhCaicuqHU%7E1P%7E5jgnuwceN3hNq1nBVTkEmQMKWZtKGRxu6Z2WzgawZGXd5ROL0fRc4fV5eYYQreq9Offfrk8ZPZsPKsjIXXFmHYsNC4oyBX4TQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\r\n",
      "--2025-06-25 01:09:35--  https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/2e2b1581a436d07a76b10b9d38765f64aa02836dc65c7dee1ce4140c11ea158b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27G48k.pth%3B+filename%3D%22G48k.pth%22%3B&Expires=1750817375&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNzM3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyLzJlMmIxNTgxYTQzNmQwN2E3NmIxMGI5ZDM4NzY1ZjY0YWEwMjgzNmRjNjVjN2RlZTFjZTQxNDBjMTFlYTE1OGI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AiuJP72hEbIA6bzeeyqVvRuAf-jWkAZUKnkeWacJzBDSpknQKQSI4OYIkIyjd3xjSXepqDNksmw4a7W1QtUmIBiTeBbKMmZuPlFbt48J7i60K-dQryEss0DEsDj5LJCBywD1epxUvD6XTx8cFl0Zgwj8S%7Ed60V%7ERZlIKSywTXs0jwPoELQs0XXdSw2cHT8%7EQJ2GYUsNPV5w1HjteDo%7EjnpoZtLwV9Qxa3MuyMClmhCaicuqHU%7E1P%7E5jgnuwceN3hNq1nBVTkEmQMKWZtKGRxu6Z2WzgawZGXd5ROL0fRc4fV5eYYQreq9Offfrk8ZPZsPKsjIXXFmHYsNC4oyBX4TQ__&Key-Pair-Id=K3RPWS32NSSJCE\r\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.239.83.31, 18.239.83.30, 18.239.83.87, ...\r\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.239.83.31|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 75318967 (72M) [binary/octet-stream]\r\n",
      "Saving to: ‘pretrained_v2/G48k.pth’\r\n",
      "\r\n",
      "G48k.pth            100%[===================>]  71.83M   332MB/s    in 0.2s    \r\n",
      "\r\n",
      "2025-06-25 01:09:36 (332 MB/s) - ‘pretrained_v2/G48k.pth’ saved [75318967/75318967]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/D48k.pth -P pretrained_v2/\n",
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/G48k.pth -P pretrained_v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9b9ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:36.269977Z",
     "iopub.status.busy": "2025-06-25T01:09:36.269665Z",
     "iopub.status.idle": "2025-06-25T01:09:43.342977Z",
     "shell.execute_reply": "2025-06-25T01:09:43.341792Z"
    },
    "papermill": {
     "duration": 7.090016,
     "end_time": "2025-06-25T01:09:43.344638",
     "exception": false,
     "start_time": "2025-06-25T01:09:36.254622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\r\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\r\n",
      "Collecting coloredlogs (from onnxruntime)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2.4.1)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\r\n",
      "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179e0359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:43.373461Z",
     "iopub.status.busy": "2025-06-25T01:09:43.373179Z",
     "iopub.status.idle": "2025-06-25T01:09:44.635034Z",
     "shell.execute_reply": "2025-06-25T01:09:44.633593Z"
    },
    "papermill": {
     "duration": 1.278495,
     "end_time": "2025-06-25T01:09:44.636804",
     "exception": false,
     "start_time": "2025-06-25T01:09:43.358309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-25 01:09:43--  https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D48k.pth\r\n",
      "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.80, 18.239.50.49, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/2269b73c7a4cf34da09aea99274dabf99b2ddb8a42cbfb065fb3c0aa9a2fc748?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27f0D48k.pth%3B+filename%3D%22f0D48k.pth%22%3B&Expires=1750816664&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNjY2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyLzIyNjliNzNjN2E0Y2YzNGRhMDlhZWE5OTI3NGRhYmY5OWIyZGRiOGE0MmNiZmIwNjVmYjNjMGFhOWEyZmM3NDg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=NnaMgYxIcZpXa6Q5HmhmFb9KgJyTce9QHDIWncJU8jktmDESkj3CZLgh1wu8N1FFK8jwn4YWPHh%7EhWm%7Ep51aJnQ1rnapM22T%7EMxZubG0QnKZ%7EnJstrOg5nZ66qXEl5AOfLf3D3ZGCfAyZvFRZjdFV8fKzym24FG9UKhKFeNmqJk43YUhyaH9NNTx-loZ%7EPXwsBGbMEeBSz778t64Q3EgAzbTjrLYax8CjZxnJ4iSJrKm83gMjjrgUY7mVcJ4cGbVfQ8eoaF2OsuEZq5qLda9RD0yBcAMOGVFReat-XiE7UAxeHOrjxqg86iEuTk9z-XzAc%7EI0nSfu1rGmfIs04mGYg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\r\n",
      "--2025-06-25 01:09:43--  https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/2269b73c7a4cf34da09aea99274dabf99b2ddb8a42cbfb065fb3c0aa9a2fc748?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27f0D48k.pth%3B+filename%3D%22f0D48k.pth%22%3B&Expires=1750816664&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNjY2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyLzIyNjliNzNjN2E0Y2YzNGRhMDlhZWE5OTI3NGRhYmY5OWIyZGRiOGE0MmNiZmIwNjVmYjNjMGFhOWEyZmM3NDg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=NnaMgYxIcZpXa6Q5HmhmFb9KgJyTce9QHDIWncJU8jktmDESkj3CZLgh1wu8N1FFK8jwn4YWPHh%7EhWm%7Ep51aJnQ1rnapM22T%7EMxZubG0QnKZ%7EnJstrOg5nZ66qXEl5AOfLf3D3ZGCfAyZvFRZjdFV8fKzym24FG9UKhKFeNmqJk43YUhyaH9NNTx-loZ%7EPXwsBGbMEeBSz778t64Q3EgAzbTjrLYax8CjZxnJ4iSJrKm83gMjjrgUY7mVcJ4cGbVfQ8eoaF2OsuEZq5qLda9RD0yBcAMOGVFReat-XiE7UAxeHOrjxqg86iEuTk9z-XzAc%7EI0nSfu1rGmfIs04mGYg__&Key-Pair-Id=K3RPWS32NSSJCE\r\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.239.83.10, 18.239.83.87, 18.239.83.30, ...\r\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.239.83.10|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 142875703 (136M) [binary/octet-stream]\r\n",
      "Saving to: ‘pretrained_v2/f0D48k.pth’\r\n",
      "\r\n",
      "f0D48k.pth          100%[===================>] 136.26M   304MB/s    in 0.4s    \r\n",
      "\r\n",
      "2025-06-25 01:09:44 (304 MB/s) - ‘pretrained_v2/f0D48k.pth’ saved [142875703/142875703]\r\n",
      "\r\n",
      "--2025-06-25 01:09:44--  https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G48k.pth\r\n",
      "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.16, 18.239.50.80, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/b5d51f589cc3632d4eae36a315b4179397695042edc01d15312e1bddc2b764a4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27f0G48k.pth%3B+filename%3D%22f0G48k.pth%22%3B&Expires=1750817384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNzM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyL2I1ZDUxZjU4OWNjMzYzMmQ0ZWFlMzZhMzE1YjQxNzkzOTc2OTUwNDJlZGMwMWQxNTMxMmUxYmRkYzJiNzY0YTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=dhTdJaK2VANHNthCA4iuBVxflNjhawmJwnbwMt6rIYSrJh4SCSMZ99hMOa5uhey1Ju0h3lbEU0acbEFidlu31ZWXWz6d4GEADkqMyNki7sGUSqWsbpOrc85Xqnw7OcsN0vBAuxLYqXHTAYWK5rMC04LpRiGfvjAIynNrauIW8AP2fN6KFULLn22ompZx9LZgm9OrfKGKkieL283ldmEh35zGHvxv15rZoKtLk8vDW8IgBtg2HWXm6HTTs-qYqSvbYK5xR7VznK2xjZprjomSaaNZPZpsLGW9McF2B%7E1Tp1LMvRoZUE4720lIHc7p0byArVRGvdWHXlmh1u5TNX89pw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\r\n",
      "--2025-06-25 01:09:44--  https://cdn-lfs.hf.co/repos/ef/97/ef977053f017cde1fc0f89ac7ef0b33172a3d8fb8a840bb24e78b1c0f35f1e72/b5d51f589cc3632d4eae36a315b4179397695042edc01d15312e1bddc2b764a4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27f0G48k.pth%3B+filename%3D%22f0G48k.pth%22%3B&Expires=1750817384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDgxNzM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lZi85Ny9lZjk3NzA1M2YwMTdjZGUxZmMwZjg5YWM3ZWYwYjMzMTcyYTNkOGZiOGE4NDBiYjI0ZTc4YjFjMGYzNWYxZTcyL2I1ZDUxZjU4OWNjMzYzMmQ0ZWFlMzZhMzE1YjQxNzkzOTc2OTUwNDJlZGMwMWQxNTMxMmUxYmRkYzJiNzY0YTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=dhTdJaK2VANHNthCA4iuBVxflNjhawmJwnbwMt6rIYSrJh4SCSMZ99hMOa5uhey1Ju0h3lbEU0acbEFidlu31ZWXWz6d4GEADkqMyNki7sGUSqWsbpOrc85Xqnw7OcsN0vBAuxLYqXHTAYWK5rMC04LpRiGfvjAIynNrauIW8AP2fN6KFULLn22ompZx9LZgm9OrfKGKkieL283ldmEh35zGHvxv15rZoKtLk8vDW8IgBtg2HWXm6HTTs-qYqSvbYK5xR7VznK2xjZprjomSaaNZPZpsLGW9McF2B%7E1Tp1LMvRoZUE4720lIHc7p0byArVRGvdWHXlmh1u5TNX89pw__&Key-Pair-Id=K3RPWS32NSSJCE\r\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.239.83.87, 18.239.83.10, 18.239.83.31, ...\r\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.239.83.87|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 75465569 (72M) [binary/octet-stream]\r\n",
      "Saving to: ‘pretrained_v2/f0G48k.pth’\r\n",
      "\r\n",
      "f0G48k.pth          100%[===================>]  71.97M   292MB/s    in 0.2s    \r\n",
      "\r\n",
      "2025-06-25 01:09:44 (292 MB/s) - ‘pretrained_v2/f0G48k.pth’ saved [75465569/75465569]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D48k.pth -P pretrained_v2/\n",
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G48k.pth -P pretrained_v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ba0694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:09:44.668293Z",
     "iopub.status.busy": "2025-06-25T01:09:44.667991Z",
     "iopub.status.idle": "2025-06-25T01:11:14.269847Z",
     "shell.execute_reply": "2025-06-25T01:11:14.268742Z"
    },
    "papermill": {
     "duration": 89.62094,
     "end_time": "2025-06-25T01:11:14.272099",
     "exception": false,
     "start_time": "2025-06-25T01:09:44.651159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchaudio scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106eb1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:14.397527Z",
     "iopub.status.busy": "2025-06-25T01:11:14.397203Z",
     "iopub.status.idle": "2025-06-25T01:11:22.548024Z",
     "shell.execute_reply": "2025-06-25T01:11:22.545506Z"
    },
    "papermill": {
     "duration": 8.213722,
     "end_time": "2025-06-25T01:11:22.551727",
     "exception": false,
     "start_time": "2025-06-25T01:11:14.338005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q av librosa numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab123cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:22.682572Z",
     "iopub.status.busy": "2025-06-25T01:11:22.682074Z",
     "iopub.status.idle": "2025-06-25T01:11:29.801164Z",
     "shell.execute_reply": "2025-06-25T01:11:29.798434Z"
    },
    "papermill": {
     "duration": 7.185351,
     "end_time": "2025-06-25T01:11:29.808673",
     "exception": false,
     "start_time": "2025-06-25T01:11:22.623322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cff593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:29.934796Z",
     "iopub.status.busy": "2025-06-25T01:11:29.934193Z",
     "iopub.status.idle": "2025-06-25T01:11:29.941234Z",
     "shell.execute_reply": "2025-06-25T01:11:29.939981Z"
    },
    "papermill": {
     "duration": 0.069633,
     "end_time": "2025-06-25T01:11:29.944070",
     "exception": false,
     "start_time": "2025-06-25T01:11:29.874437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip inastall --upgrade pip setuptools wheel\n",
    "\n",
    "# Audio + core tools\n",
    "# !pip inastall torch torchaudio scipy librosa numba faiss-cpu\n",
    "\n",
    "# !pip inastall https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/releases/download/v0.2.10/pyworld-0.2.10-cp38-cp38-manylinux1_x86_64.whl\n",
    "\n",
    "# !pip inastall praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb38963e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:30.059549Z",
     "iopub.status.busy": "2025-06-25T01:11:30.059272Z",
     "iopub.status.idle": "2025-06-25T01:11:35.514980Z",
     "shell.execute_reply": "2025-06-25T01:11:35.513921Z"
    },
    "papermill": {
     "duration": 5.515988,
     "end_time": "2025-06-25T01:11:35.518272",
     "exception": false,
     "start_time": "2025-06-25T01:11:30.002284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.9ubuntu3).\r\n",
      "libsndfile1-dev is already the newest version (1.0.31-2ubuntu0.2).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "# Essential system packages\n",
    "!apt-get install -y build-essential libsndfile1-dev\n",
    "\n",
    "# Install compatible core Python dependencies\n",
    "# !pip inastall cython==0.29.36 numpy==1.23.5\n",
    "# !pip inastall omegaconf\n",
    "# !pip inastall hydra-core\n",
    "# !pip inastall sacrebleu regex\n",
    "\n",
    "# Now install a compatible fairseq version\n",
    "# !pip inastall fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c384f109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:35.881727Z",
     "iopub.status.busy": "2025-06-25T01:11:35.881400Z",
     "iopub.status.idle": "2025-06-25T01:11:35.886397Z",
     "shell.execute_reply": "2025-06-25T01:11:35.885467Z"
    },
    "papermill": {
     "duration": 0.089077,
     "end_time": "2025-06-25T01:11:35.888716",
     "exception": false,
     "start_time": "2025-06-25T01:11:35.799639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip inastall fairseq --no-deps\n",
    "# !pip inastall omegaconf==2.0.6  # Compatible middle ground\n",
    "# !pip inastall hydra-core==1.1.0  # Newer version that works with omegaconf 2.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2672c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:36.053376Z",
     "iopub.status.busy": "2025-06-25T01:11:36.052859Z",
     "iopub.status.idle": "2025-06-25T01:11:40.664186Z",
     "shell.execute_reply": "2025-06-25T01:11:40.663230Z"
    },
    "papermill": {
     "duration": 4.695684,
     "end_time": "2025-06-25T01:11:40.665905",
     "exception": false,
     "start_time": "2025-06-25T01:11:35.970221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import intel_extension_for_pytorch as ipex\n",
    "\n",
    "    if torch.xpu.is_available():\n",
    "        from rvc.lib.ipex import ipex_init\n",
    "\n",
    "        ipex_init()\n",
    "except (ImportError, Exception):\n",
    "    pass\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# version_config_list: list = [\n",
    "#     os.path.join(root, file)\n",
    "#     # for root, dirs, files in os.walk(os.path.dirname(os.path.abspath(__file__)))\n",
    "#     for file in files\n",
    "#     if file.endswith(\".json\")\n",
    "# ]\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __new__(cls):\n",
    "        if not hasattr(cls, \"_instance\"):\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device: str = \"cuda:0\"\n",
    "        self.is_half: bool = True\n",
    "        self.use_jit: bool = False\n",
    "        self.n_cpu: int = cpu_count()\n",
    "        self.gpu_name: str | None = None\n",
    "        self.json_config = self.load_config_json()\n",
    "        self.gpu_mem: int | None = None\n",
    "        self.instead: str | None = None\n",
    "        (\n",
    "            self.python_cmd,\n",
    "            self.listen_port,\n",
    "            self.noparallel,\n",
    "            self.noautoopen,\n",
    "            self.dml,\n",
    "        ) = self.arg_parse()\n",
    "        self.x_pad, self.x_query, self.x_center, self.x_max = self.device_config()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config_json() -> dict:\n",
    "        return {\n",
    "            config_file: json.load(open(config_file, \"r\"))\n",
    "            for config_file in version_config_list\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_parse() -> tuple:\n",
    "        parser: argparse.ArgumentParser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--port\", type=int, default=7865, help=\"Listen port\")\n",
    "        parser.add_argument(\n",
    "            \"--pycmd\",\n",
    "            type=str,\n",
    "            default=sys.executable or \"python\",\n",
    "            help=\"Python command\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--noparallel\", action=\"store_true\", help=\"Disable parallel processing\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--noautoopen\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Do not open in browser automatically\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--dml\",\n",
    "            action=\"store_true\",\n",
    "            help=\"torch_dml\",\n",
    "        )\n",
    "        cmd_opts: argparse.Namespace\n",
    "        cmd_opts, _ = parser.parse_known_args()\n",
    "\n",
    "        cmd_opts.port = cmd_opts.port if 0 <= cmd_opts.port <= 65535 else 7865\n",
    "\n",
    "        return (\n",
    "            cmd_opts.pycmd,\n",
    "            cmd_opts.port,\n",
    "            cmd_opts.noparallel,\n",
    "            cmd_opts.noautoopen,\n",
    "            cmd_opts.dml,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def has_mps() -> bool:\n",
    "        return torch.backends.mps.is_available() and not torch.zeros(1).to(\n",
    "            torch.device(\"mps\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def has_xpu() -> bool:\n",
    "        return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n",
    "\n",
    "    def params_config(self) -> tuple:\n",
    "        if self.gpu_mem is not None and self.gpu_mem <= 4:\n",
    "            x_pad = 1\n",
    "            x_query = 5\n",
    "            x_center = 30\n",
    "            x_max = 32\n",
    "        elif self.is_half:\n",
    "            # 6G PU_RAM conf\n",
    "            x_pad = 3\n",
    "            x_query = 10\n",
    "            x_center = 60\n",
    "            x_max = 65\n",
    "        else:\n",
    "            # 5G GPU_RAM conf\n",
    "            x_pad = 1\n",
    "            x_query = 6\n",
    "            x_center = 38\n",
    "            x_max = 41\n",
    "        return x_pad, x_query, x_center, x_max\n",
    "\n",
    "    def use_cuda(self) -> None:\n",
    "        if self.has_xpu():\n",
    "            self.device = self.instead = \"xpu:0\"\n",
    "            self.is_half = True\n",
    "        i_device = int(self.device.split(\":\")[-1])\n",
    "        self.gpu_name = torch.cuda.get_device_name(i_device)\n",
    "        if (\n",
    "            (\"16\" in self.gpu_name and \"V100\" not in self.gpu_name.upper())\n",
    "            or \"P40\" in self.gpu_name.upper()\n",
    "            or \"P10\" in self.gpu_name.upper()\n",
    "            or \"1060\" in self.gpu_name\n",
    "            or \"1070\" in self.gpu_name\n",
    "            or \"1080\" in self.gpu_name\n",
    "        ):\n",
    "            logger.info(f\"Found GPU {self.gpu_name}, force to fp32\")\n",
    "            self.is_half = False\n",
    "            self.use_fp32_config()\n",
    "        else:\n",
    "            logger.info(f\"Found GPU {self.gpu_name}\")\n",
    "        self.gpu_mem = int(\n",
    "            torch.cuda.get_device_properties(i_device).total_memory / 1024 / 1024 / 1024\n",
    "            + 0.4\n",
    "        )\n",
    "\n",
    "    def use_mps(self) -> None:\n",
    "        self.device = self.instead = \"mps\"\n",
    "        self.is_half = False\n",
    "        self.use_fp32_config()\n",
    "        self.params_config()\n",
    "\n",
    "    def use_dml(self) -> None:\n",
    "        import torch_directml\n",
    "\n",
    "        self.device = torch_directml.device(torch_directml.default_device())\n",
    "        self.is_half = False\n",
    "        self.params_config()\n",
    "\n",
    "    def use_cpu(self) -> None:\n",
    "        self.device = self.instead = \"cpu\"\n",
    "        self.is_half = False\n",
    "        self.use_fp32_config()\n",
    "        self.params_config()\n",
    "\n",
    "    def use_fp32_config(self) -> None:\n",
    "        for config_file, data in self.json_config.items():\n",
    "            try:\n",
    "                data[\"train\"][\"fp16_run\"] = False\n",
    "                with open(config_file, \"w\") as json_file:\n",
    "                    json.dump(data, json_file, indent=4)\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error updating {config_file}: {str(e)}\")\n",
    "        logger.info(\"overwrite configs.json\")\n",
    "\n",
    "    def device_config(self) -> tuple:\n",
    "        if torch.cuda.is_available():\n",
    "            self.use_cuda()\n",
    "        elif self.has_mps():\n",
    "            logger.info(\"No supported Nvidia GPU found\")\n",
    "            self.use_mps()\n",
    "        elif self.dml:\n",
    "            self.use_dml()\n",
    "        else:\n",
    "            logger.info(\"No supported Nvidia GPU found\")\n",
    "            self.device = self.instead = \"cpu\"\n",
    "            self.is_half = False\n",
    "            self.use_fp32_config()\n",
    "\n",
    "        logger.info(f\"Use {self.dml or self.instead} instead\")\n",
    "        logger.info(f\"is_half:{self.is_half}, device:{self.device}\")\n",
    "        return self.params_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf7347b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:40.758331Z",
     "iopub.status.busy": "2025-06-25T01:11:40.757800Z",
     "iopub.status.idle": "2025-06-25T01:11:44.469656Z",
     "shell.execute_reply": "2025-06-25T01:11:44.468331Z"
    },
    "papermill": {
     "duration": 3.760972,
     "end_time": "2025-06-25T01:11:44.472004",
     "exception": false,
     "start_time": "2025-06-25T01:11:40.711032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (14.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install av --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc439f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:44.561393Z",
     "iopub.status.busy": "2025-06-25T01:11:44.561081Z",
     "iopub.status.idle": "2025-06-25T01:11:44.649953Z",
     "shell.execute_reply": "2025-06-25T01:11:44.648640Z"
    },
    "papermill": {
     "duration": 0.134397,
     "end_time": "2025-06-25T01:11:44.651313",
     "exception": false,
     "start_time": "2025-06-25T01:11:44.516916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyAV version: 14.4.0\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "print(f\"PyAV version: {av.__version__}\")  # Should be >= 9.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6aa432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:44.735426Z",
     "iopub.status.busy": "2025-06-25T01:11:44.734852Z",
     "iopub.status.idle": "2025-06-25T01:11:44.752266Z",
     "shell.execute_reply": "2025-06-25T01:11:44.751359Z"
    },
    "papermill": {
     "duration": 0.061625,
     "end_time": "2025-06-25T01:11:44.753850",
     "exception": false,
     "start_time": "2025-06-25T01:11:44.692225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import av  # PyAV for audio processing\n",
    "import os\n",
    "import numpy as np\n",
    "import av\n",
    "\n",
    "def load_audio(file_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Robust audio loading function with multiple fallbacks: PyAV, librosa, scipy.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to audio file.\n",
    "        target_sr (int): Desired sample rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Loaded audio data (mono, float32).\n",
    "    \"\"\"\n",
    "    if not isinstance(file_path, str) or not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "\n",
    "    # Try PyAV\n",
    "    try:\n",
    "        with av.open(str(file_path), 'r') as container:\n",
    "            stream = container.streams.audio[0]\n",
    "            stream.thread_type = 'AUTO'\n",
    "            audio_frames = []\n",
    "            for frame in container.decode(stream):\n",
    "                frame_array = frame.to_ndarray()\n",
    "                if frame_array.ndim == 2:\n",
    "                    frame_array = frame_array.mean(axis=0)\n",
    "                audio_frames.append(frame_array)\n",
    "            audio = np.concatenate(audio_frames)\n",
    "            return audio.astype(np.float32)\n",
    "\n",
    "    except Exception as av_error:\n",
    "        pass  # Continue to librosa fallback\n",
    "\n",
    "    # Try Librosa\n",
    "    try:\n",
    "        import librosa\n",
    "        audio, _ = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        return audio.astype(np.float32)\n",
    "\n",
    "    except Exception as librosa_error:\n",
    "        pass  # Continue to scipy fallback\n",
    "\n",
    "    # Try Scipy\n",
    "    try:\n",
    "        from scipy.io import wavfile\n",
    "        sr, audio = wavfile.read(file_path)\n",
    "        if audio.dtype == np.int16:\n",
    "            audio = audio.astype(np.float32) / 32768.0\n",
    "        elif audio.dtype == np.int32:\n",
    "            audio = audio.astype(np.float32) / 2147483648.0\n",
    "        elif audio.dtype != np.float32:\n",
    "            audio = audio.astype(np.float32)\n",
    "        return audio\n",
    "\n",
    "    except Exception as scipy_error:\n",
    "        raise RuntimeError(\n",
    "            f\"All audio loading methods failed for: {file_path}\\n\"\n",
    "            f\"PyAV Error: {av_error}\\n\"\n",
    "            f\"Librosa Error: {librosa_error}\\n\"\n",
    "            f\"Scipy Error: {scipy_error}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_audio(input_path: str, output_path: str, format: str, sr: int = None):\n",
    "    \"\"\"\n",
    "    Convert audio format using PyAV with proper mode handling.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Input audio file path\n",
    "        output_path: Output audio file path\n",
    "        format: Output format (ogg, mp3, wav, f32le, etc.)\n",
    "        sr: Optional target sample rate\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If conversion fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with av.open(str(input_path), 'r') as inp:\n",
    "            with av.open(str(output_path), 'w') as out:\n",
    "                # Handle format aliases\n",
    "                if format == \"ogg\":\n",
    "                    codec = \"libvorbis\"\n",
    "                elif format == \"f32le\":\n",
    "                    codec = \"pcm_f32le\"\n",
    "                elif format == \"m4a\":\n",
    "                    codec = \"aac\"\n",
    "                    format = \"mp4\"\n",
    "                else:\n",
    "                    codec = format\n",
    "\n",
    "                ostream = out.add_stream(codec)\n",
    "                if sr is not None:\n",
    "                    ostream.sample_rate = sr\n",
    "\n",
    "                for frame in inp.decode(audio=0):\n",
    "                    frame.pts = None  # Reset timestamps\n",
    "                    for packet in ostream.encode(frame):\n",
    "                        out.mux(packet)\n",
    "\n",
    "                # Flush the encoder\n",
    "                for packet in ostream.encode(None):\n",
    "                    out.mux(packet)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Audio conversion failed: {str(e)}\")\n",
    "\n",
    "def wav2(input_path: str, output_path: str, format: str):\n",
    "    \"\"\"\n",
    "    Special case for WAV conversion with default sample rate.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Input WAV file path\n",
    "        output_path: Output file path\n",
    "        format: Output format\n",
    "    \"\"\"\n",
    "    convert_audio(input_path, output_path, format, sr=44100)\n",
    "\n",
    "def load_audio_via_tempfile(file_path: str, sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Alternative audio loading method that converts to f32le first.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file\n",
    "        sr: Target sample rate\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Audio samples as float32 array\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If conversion fails\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".f32le\") as tmp:\n",
    "            convert_audio(file_path, tmp.name, \"f32le\", sr)\n",
    "            tmp.seek(0)\n",
    "            audio = np.frombuffer(tmp.read(), dtype=np.float32).flatten()\n",
    "            return audio\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio via tempfile: {str(e)}\\n{traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13371f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:44.841804Z",
     "iopub.status.busy": "2025-06-25T01:11:44.841439Z",
     "iopub.status.idle": "2025-06-25T01:11:44.898252Z",
     "shell.execute_reply": "2025-06-25T01:11:44.897222Z"
    },
    "papermill": {
     "duration": 0.102912,
     "end_time": "2025-06-25T01:11:44.900019",
     "exception": false,
     "start_time": "2025-06-25T01:11:44.797107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size * dilation - dilation) / 2)\n",
    "\n",
    "\n",
    "# def convert_pad_shape(pad_shape):\n",
    "#     l = pad_shape[::-1]\n",
    "#     pad_shape = [item for sublist in l for item in sublist]\n",
    "#     return pad_shape\n",
    "\n",
    "\n",
    "def kl_divergence(m_p, logs_p, m_q, logs_q):\n",
    "    \"\"\"KL(P||Q)\"\"\"\n",
    "    kl = (logs_q - logs_p) - 0.5\n",
    "    kl += (\n",
    "        0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n",
    "    )\n",
    "    return kl\n",
    "\n",
    "\n",
    "def rand_gumbel(shape):\n",
    "    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n",
    "    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n",
    "    return -torch.log(-torch.log(uniform_samples))\n",
    "\n",
    "\n",
    "def rand_gumbel_like(x):\n",
    "    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n",
    "    return g\n",
    "\n",
    "\n",
    "def slice_segments(x, ids_str, segment_size=4):\n",
    "    ret = torch.zeros_like(x[:, :, :segment_size])\n",
    "    for i in range(x.size(0)):\n",
    "        idx_str = ids_str[i]\n",
    "        idx_end = idx_str + segment_size\n",
    "        ret[i] = x[i, :, idx_str:idx_end]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def slice_segments2(x, ids_str, segment_size=4):\n",
    "    ret = torch.zeros_like(x[:, :segment_size])\n",
    "    for i in range(x.size(0)):\n",
    "        idx_str = ids_str[i]\n",
    "        idx_end = idx_str + segment_size\n",
    "        ret[i] = x[i, idx_str:idx_end]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def rand_slice_segments(x, x_lengths=None, segment_size=4):\n",
    "    b, d, t = x.size()\n",
    "    if x_lengths is None:\n",
    "        x_lengths = t\n",
    "    ids_str_max = x_lengths - segment_size + 1\n",
    "    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n",
    "    ret = slice_segments(x, ids_str, segment_size)\n",
    "    return ret, ids_str\n",
    "\n",
    "\n",
    "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    position = torch.arange(length, dtype=torch.float)\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (\n",
    "        num_timescales - 1\n",
    "    )\n",
    "    inv_timescales = min_timescale * torch.exp(\n",
    "        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n",
    "    )\n",
    "    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n",
    "    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n",
    "    signal = F.pad(signal, [0, 0, 0, channels % 2])\n",
    "    signal = signal.view(1, channels, length)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    b, channels, length = x.size()\n",
    "    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n",
    "    return x + signal.to(dtype=x.dtype, device=x.device)\n",
    "\n",
    "\n",
    "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n",
    "    b, channels, length = x.size()\n",
    "    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n",
    "    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n",
    "\n",
    "\n",
    "def subsequent_mask(length):\n",
    "    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    in_act = input_a + input_b\n",
    "    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n",
    "    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n",
    "    acts = t_act * s_act\n",
    "    return acts\n",
    "\n",
    "\n",
    "# def convert_pad_shape(pad_shape):\n",
    "#     l = pad_shape[::-1]\n",
    "#     pad_shape = [item for sublist in l for item in sublist]\n",
    "#     return pad_shape\n",
    "\n",
    "\n",
    "def convert_pad_shape(pad_shape: List[List[int]]) -> List[int]:\n",
    "    return torch.tensor(pad_shape).flip(0).reshape(-1).int().tolist()\n",
    "\n",
    "\n",
    "def shift_1d(x):\n",
    "    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "def sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "    \"\"\"\n",
    "    duration: [b, 1, t_x]\n",
    "    mask: [b, 1, t_y, t_x]\n",
    "    \"\"\"\n",
    "    device = duration.device\n",
    "\n",
    "    b, _, t_y, t_x = mask.shape\n",
    "    cum_duration = torch.cumsum(duration, -1)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n",
    "    path = path.view(b, t_x, t_y)\n",
    "    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n",
    "    path = path.unsqueeze(1).transpose(2, 3) * mask\n",
    "    return path\n",
    "\n",
    "\n",
    "def clip_grad_value_(parameters, clip_value, norm_type=2):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    if clip_value is not None:\n",
    "        clip_value = float(clip_value)\n",
    "\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "        if clip_value is not None:\n",
    "            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n",
    "    total_norm = total_norm ** (1.0 / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3bfb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:44.991820Z",
     "iopub.status.busy": "2025-06-25T01:11:44.991528Z",
     "iopub.status.idle": "2025-06-25T01:11:45.015244Z",
     "shell.execute_reply": "2025-06-25T01:11:45.013982Z"
    },
    "papermill": {
     "duration": 0.070325,
     "end_time": "2025-06-25T01:11:45.016951",
     "exception": false,
     "start_time": "2025-06-25T01:11:44.946626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size * dilation - dilation) / 2)\n",
    "\n",
    "\n",
    "# def convert_pad_shape(pad_shape):\n",
    "#     l = pad_shape[::-1]\n",
    "#     pad_shape = [item for sublist in l for item in sublist]\n",
    "#     return pad_shape\n",
    "\n",
    "\n",
    "def kl_divergence(m_p, logs_p, m_q, logs_q):\n",
    "    \"\"\"KL(P||Q)\"\"\"\n",
    "    kl = (logs_q - logs_p) - 0.5\n",
    "    kl += (\n",
    "        0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n",
    "    )\n",
    "    return kl\n",
    "\n",
    "\n",
    "def rand_gumbel(shape):\n",
    "    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n",
    "    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n",
    "    return -torch.log(-torch.log(uniform_samples))\n",
    "\n",
    "\n",
    "def rand_gumbel_like(x):\n",
    "    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n",
    "    return g\n",
    "\n",
    "\n",
    "def slice_segments(x, ids_str, segment_size=4):\n",
    "    ret = torch.zeros_like(x[:, :, :segment_size])\n",
    "    for i in range(x.size(0)):\n",
    "        idx_str = ids_str[i]\n",
    "        idx_end = idx_str + segment_size\n",
    "        ret[i] = x[i, :, idx_str:idx_end]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def slice_segments2(x, ids_str, segment_size=4):\n",
    "    ret = torch.zeros_like(x[:, :segment_size])\n",
    "    for i in range(x.size(0)):\n",
    "        idx_str = ids_str[i]\n",
    "        idx_end = idx_str + segment_size\n",
    "        ret[i] = x[i, idx_str:idx_end]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def rand_slice_segments(x, x_lengths=None, segment_size=4):\n",
    "    b, d, t = x.size()\n",
    "    if x_lengths is None:\n",
    "        x_lengths = t\n",
    "    ids_str_max = x_lengths - segment_size + 1\n",
    "    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n",
    "    ret = slice_segments(x, ids_str, segment_size)\n",
    "    return ret, ids_str\n",
    "\n",
    "\n",
    "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    position = torch.arange(length, dtype=torch.float)\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (\n",
    "        num_timescales - 1\n",
    "    )\n",
    "    inv_timescales = min_timescale * torch.exp(\n",
    "        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n",
    "    )\n",
    "    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n",
    "    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n",
    "    signal = F.pad(signal, [0, 0, 0, channels % 2])\n",
    "    signal = signal.view(1, channels, length)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    b, channels, length = x.size()\n",
    "    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n",
    "    return x + signal.to(dtype=x.dtype, device=x.device)\n",
    "\n",
    "\n",
    "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n",
    "    b, channels, length = x.size()\n",
    "    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n",
    "    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n",
    "\n",
    "\n",
    "def subsequent_mask(length):\n",
    "    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    in_act = input_a + input_b\n",
    "    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n",
    "    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n",
    "    acts = t_act * s_act\n",
    "    return acts\n",
    "\n",
    "\n",
    "# def convert_pad_shape(pad_shape):\n",
    "#     l = pad_shape[::-1]\n",
    "#     pad_shape = [item for sublist in l for item in sublist]\n",
    "#     return pad_shape\n",
    "\n",
    "\n",
    "def convert_pad_shape(pad_shape: List[List[int]]) -> List[int]:\n",
    "    return torch.tensor(pad_shape).flip(0).reshape(-1).int().tolist()\n",
    "\n",
    "\n",
    "def shift_1d(x):\n",
    "    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "def sequence_mask(length: torch.Tensor, max_length: Optional[int] = None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "    \"\"\"\n",
    "    duration: [b, 1, t_x]\n",
    "    mask: [b, 1, t_y, t_x]\n",
    "    \"\"\"\n",
    "    device = duration.device\n",
    "\n",
    "    b, _, t_y, t_x = mask.shape\n",
    "    cum_duration = torch.cumsum(duration, -1)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n",
    "    path = path.view(b, t_x, t_y)\n",
    "    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n",
    "    path = path.unsqueeze(1).transpose(2, 3) * mask\n",
    "    return path\n",
    "\n",
    "\n",
    "def clip_grad_value_(parameters, clip_value, norm_type=2):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    if clip_value is not None:\n",
    "        clip_value = float(clip_value)\n",
    "\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "        if clip_value is not None:\n",
    "            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n",
    "    total_norm = total_norm ** (1.0 / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "274663ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:45.108697Z",
     "iopub.status.busy": "2025-06-25T01:11:45.107166Z",
     "iopub.status.idle": "2025-06-25T01:11:45.150087Z",
     "shell.execute_reply": "2025-06-25T01:11:45.148996Z"
    },
    "papermill": {
     "duration": 0.091558,
     "end_time": "2025-06-25T01:11:45.151911",
     "exception": false,
     "start_time": "2025-06-25T01:11:45.060353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from rvc.lib.infer_pack import commons, modules\n",
    "# from rvc.lib.infer_pack.modules import LayerNorm\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size=1,\n",
    "        p_dropout=0.0,\n",
    "        window_size=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = int(n_layers)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "        self.attn_layers = nn.ModuleList()\n",
    "        self.norm_layers_1 = nn.ModuleList()\n",
    "        self.ffn_layers = nn.ModuleList()\n",
    "        self.norm_layers_2 = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    n_heads,\n",
    "                    p_dropout=p_dropout,\n",
    "                    window_size=window_size,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "            self.ffn_layers.append(\n",
    "                FFN(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    filter_channels,\n",
    "                    kernel_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "        x = x * x_mask\n",
    "        zippep = zip(\n",
    "            self.attn_layers, self.norm_layers_1, self.ffn_layers, self.norm_layers_2\n",
    "        )\n",
    "        for attn_layers, norm_layers_1, ffn_layers, norm_layers_2 in zippep:\n",
    "            y = attn_layers(x, x, attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = norm_layers_1(x + y)\n",
    "\n",
    "            y = ffn_layers(x, x_mask)\n",
    "            y = self.drop(y)\n",
    "            x = norm_layers_2(x + y)\n",
    "        x = x * x_mask\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size=1,\n",
    "        p_dropout=0.0,\n",
    "        proximal_bias=False,\n",
    "        proximal_init=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.proximal_bias = proximal_bias\n",
    "        self.proximal_init = proximal_init\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "        self.self_attn_layers = nn.ModuleList()\n",
    "        self.norm_layers_0 = nn.ModuleList()\n",
    "        self.encdec_attn_layers = nn.ModuleList()\n",
    "        self.norm_layers_1 = nn.ModuleList()\n",
    "        self.ffn_layers = nn.ModuleList()\n",
    "        self.norm_layers_2 = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.self_attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    n_heads,\n",
    "                    p_dropout=p_dropout,\n",
    "                    proximal_bias=proximal_bias,\n",
    "                    proximal_init=proximal_init,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_0.append(LayerNorm(hidden_channels))\n",
    "            self.encdec_attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "            self.ffn_layers.append(\n",
    "                FFN(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    filter_channels,\n",
    "                    kernel_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                    causal=True,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, x_mask, h, h_mask):\n",
    "        \"\"\"\n",
    "        x: decoder input\n",
    "        h: encoder output\n",
    "        \"\"\"\n",
    "        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n",
    "            device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "        x = x * x_mask\n",
    "        for i in range(self.n_layers):\n",
    "            y = self.self_attn_layers[i](x, x, self_attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_0[i](x + y)\n",
    "\n",
    "            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "            y = self.ffn_layers[i](x, x_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_2[i](x + y)\n",
    "        x = x * x_mask\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        out_channels,\n",
    "        n_heads,\n",
    "        p_dropout=0.0,\n",
    "        window_size=None,\n",
    "        heads_share=True,\n",
    "        block_length=None,\n",
    "        proximal_bias=False,\n",
    "        proximal_init=False,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert channels % n_heads == 0\n",
    "\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        self.window_size = window_size\n",
    "        self.heads_share = heads_share\n",
    "        self.block_length = block_length\n",
    "        self.proximal_bias = proximal_bias\n",
    "        self.proximal_init = proximal_init\n",
    "        self.attn = None\n",
    "\n",
    "        self.k_channels = channels // n_heads\n",
    "        self.conv_q = nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_k = nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_v = nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_o = nn.Conv1d(channels, out_channels, 1)\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "        if window_size is not None:\n",
    "            n_heads_rel = 1 if heads_share else n_heads\n",
    "            rel_stddev = self.k_channels**-0.5\n",
    "            self.emb_rel_k = nn.Parameter(\n",
    "                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n",
    "                * rel_stddev\n",
    "            )\n",
    "            self.emb_rel_v = nn.Parameter(\n",
    "                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n",
    "                * rel_stddev\n",
    "            )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "        if proximal_init:\n",
    "            with torch.no_grad():\n",
    "                self.conv_k.weight.copy_(self.conv_q.weight)\n",
    "                self.conv_k.bias.copy_(self.conv_q.bias)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, c: torch.Tensor, attn_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "\n",
    "        x, _ = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        # reshape [b, d, t] -> [b, n_h, t, d_k]\n",
    "        b, d, t_s = key.size()\n",
    "        t_t = query.size(2)\n",
    "        query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n",
    "        key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n",
    "        value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n",
    "\n",
    "        scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n",
    "        if self.window_size is not None:\n",
    "            assert (\n",
    "                t_s == t_t\n",
    "            ), \"Relative attention is only available for self-attention.\"\n",
    "            key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "            rel_logits = self._matmul_with_relative_keys(\n",
    "                query / math.sqrt(self.k_channels), key_relative_embeddings\n",
    "            )\n",
    "            scores_local = self._relative_position_to_absolute_position(rel_logits)\n",
    "            scores = scores + scores_local\n",
    "        if self.proximal_bias:\n",
    "            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n",
    "            scores = scores + self._attention_bias_proximal(t_s).to(\n",
    "                device=scores.device, dtype=scores.dtype\n",
    "            )\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "            if self.block_length is not None:\n",
    "                assert (\n",
    "                    t_s == t_t\n",
    "                ), \"Local attention is only available for self-attention.\"\n",
    "                block_mask = (\n",
    "                    torch.ones_like(scores)\n",
    "                    .triu(-self.block_length)\n",
    "                    .tril(self.block_length)\n",
    "                )\n",
    "                scores = scores.masked_fill(block_mask == 0, -1e4)\n",
    "        p_attn = F.softmax(scores, dim=-1)  # [b, n_h, t_t, t_s]\n",
    "        p_attn = self.drop(p_attn)\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        if self.window_size is not None:\n",
    "            relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "            value_relative_embeddings = self._get_relative_embeddings(\n",
    "                self.emb_rel_v, t_s\n",
    "            )\n",
    "            output = output + self._matmul_with_relative_values(\n",
    "                relative_weights, value_relative_embeddings\n",
    "            )\n",
    "        output = (\n",
    "            output.transpose(2, 3).contiguous().view(b, d, t_t)\n",
    "        )  # [b, n_h, t_t, d_k] -> [b, d, t_t]\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, m]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, d]\n",
    "        \"\"\"\n",
    "        ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, d]\n",
    "        y: [h or 1, m, d]\n",
    "        ret: [b, h, l, m]\n",
    "        \"\"\"\n",
    "        ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length: int):\n",
    "        max_relative_position = 2 * self.window_size + 1\n",
    "        # Pad first before slice to avoid using cond ops.\n",
    "        pad_length: int = max(length - (self.window_size + 1), 0)\n",
    "        slice_start_position = max((self.window_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = F.pad(\n",
    "                relative_embeddings,\n",
    "                # commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),\n",
    "                [0, 0, pad_length, pad_length, 0, 0],\n",
    "            )\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[\n",
    "            :, slice_start_position:slice_end_position\n",
    "        ]\n",
    "        return used_relative_embeddings\n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, 2*l-1]\n",
    "        ret: [b, h, l, l]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # Concat columns of pad to shift from relative to absolute indexing.\n",
    "        x = F.pad(\n",
    "            x,\n",
    "            #   commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        )\n",
    "\n",
    "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = F.pad(\n",
    "            x_flat,\n",
    "            # commons.convert_pad_shape([[0, 0], [0, 0], [0, int(length) - 1]])\n",
    "            [0, int(length) - 1, 0, 0, 0, 0],\n",
    "        )\n",
    "\n",
    "        # Reshape and slice out the padded elements.\n",
    "        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[\n",
    "            :, :, :length, length - 1 :\n",
    "        ]\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x):\n",
    "        \"\"\"\n",
    "        x: [b, h, l, l]\n",
    "        ret: [b, h, l, 2*l-1]\n",
    "        \"\"\"\n",
    "        batch, heads, length, _ = x.size()\n",
    "        # padd along column\n",
    "        x = F.pad(\n",
    "            x,\n",
    "            # commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, int(length) - 1]])\n",
    "            [0, int(length) - 1, 0, 0, 0, 0, 0, 0],\n",
    "        )\n",
    "        x_flat = x.view([batch, heads, int(length**2) + int(length * (length - 1))])\n",
    "        # add 0's in the beginning that will skew the elements after reshape\n",
    "        x_flat = F.pad(\n",
    "            x_flat,\n",
    "            #    commons.convert_pad_shape([[0, 0], [0, 0], [int(length), 0]])\n",
    "            [length, 0, 0, 0, 0, 0],\n",
    "        )\n",
    "        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n",
    "        return x_final\n",
    "\n",
    "    def _attention_bias_proximal(self, length: int):\n",
    "        \"\"\"Bias for self-attention to encourage attention to close positions.\n",
    "        Args:\n",
    "          length: an integer scalar.\n",
    "        Returns:\n",
    "          a Tensor with shape [1, 1, length, length]\n",
    "        \"\"\"\n",
    "        r = torch.arange(length, dtype=torch.float32)\n",
    "        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n",
    "        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        filter_channels,\n",
    "        kernel_size,\n",
    "        p_dropout=0.0,\n",
    "        activation: str = None,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super(FFN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.activation = activation\n",
    "        self.causal = causal\n",
    "        self.is_activation = True if activation == \"gelu\" else False\n",
    "        # if causal:\n",
    "        #     self.padding = self._causal_padding\n",
    "        # else:\n",
    "        #     self.padding = self._same_padding\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n",
    "        self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "\n",
    "    def padding(self, x: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:\n",
    "        if self.causal:\n",
    "            padding = self._causal_padding(x * x_mask)\n",
    "        else:\n",
    "            padding = self._same_padding(x * x_mask)\n",
    "        return padding\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_mask: torch.Tensor):\n",
    "        x = self.conv_1(self.padding(x, x_mask))\n",
    "        if self.is_activation:\n",
    "            x = x * torch.sigmoid(1.702 * x)\n",
    "        else:\n",
    "            x = torch.relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.conv_2(self.padding(x, x_mask))\n",
    "        return x * x_mask\n",
    "\n",
    "    def _causal_padding(self, x):\n",
    "        if self.kernel_size == 1:\n",
    "            return x\n",
    "        pad_l: int = self.kernel_size - 1\n",
    "        pad_r: int = 0\n",
    "        # padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "        x = F.pad(\n",
    "            x,\n",
    "            #   commons.convert_pad_shape(padding)\n",
    "            [pad_l, pad_r, 0, 0, 0, 0],\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def _same_padding(self, x):\n",
    "        if self.kernel_size == 1:\n",
    "            return x\n",
    "        pad_l: int = (self.kernel_size - 1) // 2\n",
    "        pad_r: int = self.kernel_size // 2\n",
    "        # padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n",
    "        x = F.pad(\n",
    "            x,\n",
    "            #   commons.convert_pad_shape(padding)\n",
    "            [pad_l, pad_r, 0, 0, 0, 0],\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c64236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:45.237495Z",
     "iopub.status.busy": "2025-06-25T01:11:45.237231Z",
     "iopub.status.idle": "2025-06-25T01:11:45.384913Z",
     "shell.execute_reply": "2025-06-25T01:11:45.383849Z"
    },
    "papermill": {
     "duration": 0.193473,
     "end_time": "2025-06-25T01:11:45.386800",
     "exception": false,
     "start_time": "2025-06-25T01:11:45.193327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\n",
    "\n",
    "# from rvc.lib.infer_pack import attentions, commons, modules\n",
    "# from rvc.lib.infer_pack.commons import get_padding, init_weights\n",
    "\n",
    "has_xpu = bool(hasattr(torch, \"xpu\") and torch.xpu.is_available())\n",
    "\n",
    "\n",
    "class TextEncoder256(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        f0=True,\n",
    "    ):\n",
    "        super(TextEncoder256, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.emb_phone = nn.Linear(256, hidden_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n",
    "        if f0 == True:\n",
    "            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n",
    "        self.encoder = Encoder(\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, phone: torch.Tensor, pitch: Optional[torch.Tensor], lengths: torch.Tensor\n",
    "    ):\n",
    "        if pitch is None:\n",
    "            x = self.emb_phone(phone)\n",
    "        else:\n",
    "            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n",
    "        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
    "        x = self.lrelu(x)\n",
    "        x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.encoder(x * x_mask, x_mask)\n",
    "        stats = self.proj(x) * x_mask\n",
    "\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        return m, logs, x_mask\n",
    "\n",
    "\n",
    "class TextEncoder768(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        f0=True,\n",
    "    ):\n",
    "        super(TextEncoder768, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.emb_phone = nn.Linear(768, hidden_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n",
    "        if f0 == True:\n",
    "            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n",
    "        self.encoder = Encoder(\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(self, phone: torch.Tensor, pitch: torch.Tensor, lengths: torch.Tensor):\n",
    "        if pitch is None:\n",
    "            x = self.emb_phone(phone)\n",
    "        else:\n",
    "            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n",
    "        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
    "        x = self.lrelu(x)\n",
    "        x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.encoder(x * x_mask, x_mask)\n",
    "        stats = self.proj(x) * x_mask\n",
    "\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        return m, logs, x_mask\n",
    "\n",
    "\n",
    "class ResidualCouplingBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        n_flows=4,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(ResidualCouplingBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.n_flows = n_flows\n",
    "        self.gin_channels = gin_channels\n",
    "\n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(n_flows):\n",
    "            self.flows.append(\n",
    "                ResidualCouplingLayer(\n",
    "                    channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size,\n",
    "                    dilation_rate,\n",
    "                    n_layers,\n",
    "                    gin_channels=gin_channels,\n",
    "                    mean_only=True,\n",
    "                )\n",
    "            )\n",
    "            self.flows.append(Flip())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse: bool = False,\n",
    "    ):\n",
    "        if not reverse:\n",
    "            for flow in self.flows:\n",
    "                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n",
    "        else:\n",
    "            for flow in self.flows[::-1]:\n",
    "                x, _ = flow.forward(x, x_mask, g=g, reverse=reverse)\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for i in range(self.n_flows):\n",
    "            self.flows[i * 2].remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for i in range(self.n_flows):\n",
    "            for hook in self.flows[i * 2]._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.flows[i * 2])\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class PosteriorEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(PosteriorEncoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.gin_channels = gin_channels\n",
    "\n",
    "        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n",
    "        self.enc = WN(\n",
    "            hidden_channels,\n",
    "            kernel_size,\n",
    "            dilation_rate,\n",
    "            n_layers,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, x_lengths: torch.Tensor, g: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.pre(x) * x_mask\n",
    "        x = self.enc(x, x_mask, g=g)\n",
    "        stats = self.proj(x) * x_mask\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "        return z, m, logs, x_mask\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.enc.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.enc._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.enc)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_channel,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(\n",
    "            initial_channel, upsample_initial_channel, 7, 1, padding=3\n",
    "        )\n",
    "        resblock = ResBlock1 if resblock == \"1\" else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(\n",
    "                weight_norm(\n",
    "                    ConvTranspose1d(\n",
    "                        upsample_initial_channel // (2**i),\n",
    "                        upsample_initial_channel // (2 ** (i + 1)),\n",
    "                        k,\n",
    "                        u,\n",
    "                        padding=(k - u) // 2,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2 ** (i + 1))\n",
    "            for j, (k, d) in enumerate(\n",
    "                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n",
    "            ):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, g: Optional[torch.Tensor] = None):\n",
    "        x = self.conv_pre(x)\n",
    "        if g is not None:\n",
    "            x = x + self.cond(g)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i * self.num_kernels + j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i * self.num_kernels + j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.ups:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                # The hook we want to remove is an instance of WeightNorm class, so\n",
    "                # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "                # because of shadowing, so we check the module name directly.\n",
    "                # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "\n",
    "        for l in self.resblocks:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.ups:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "\n",
    "\n",
    "class SineGen(torch.nn.Module):\n",
    "    \"\"\"Definition of sine generator\n",
    "    SineGen(samp_rate, harmonic_num = 0,\n",
    "            sine_amp = 0.1, noise_std = 0.003,\n",
    "            voiced_threshold = 0,\n",
    "            flag_for_pulse=False)\n",
    "    samp_rate: sampling rate in Hz\n",
    "    harmonic_num: number of harmonic overtones (default 0)\n",
    "    sine_amp: amplitude of sine-wavefrom (default 0.1)\n",
    "    noise_std: std of Gaussian noise (default 0.003)\n",
    "    voiced_thoreshold: F0 threshold for U/V classification (default 0)\n",
    "    flag_for_pulse: this SinGen is used inside PulseGen (default False)\n",
    "    Note: when flag_for_pulse is True, the first time step of a voiced\n",
    "        segment is always sin(torch.pi) or cos(0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        samp_rate,\n",
    "        harmonic_num=0,\n",
    "        sine_amp=0.1,\n",
    "        noise_std=0.003,\n",
    "        voiced_threshold=0,\n",
    "        flag_for_pulse=False,\n",
    "    ):\n",
    "        super(SineGen, self).__init__()\n",
    "        self.sine_amp = sine_amp\n",
    "        self.noise_std = noise_std\n",
    "        self.harmonic_num = harmonic_num\n",
    "        self.dim = self.harmonic_num + 1\n",
    "        self.sampling_rate = samp_rate\n",
    "        self.voiced_threshold = voiced_threshold\n",
    "\n",
    "    def _f02uv(self, f0):\n",
    "        # generate uv signal\n",
    "        uv = torch.ones_like(f0)\n",
    "        uv = uv * (f0 > self.voiced_threshold)\n",
    "        if uv.device.type == \"privateuseone\":  # for DirectML\n",
    "            uv = uv.float()\n",
    "        return uv\n",
    "\n",
    "    def forward(self, f0: torch.Tensor, upp: int):\n",
    "        \"\"\"sine_tensor, uv = forward(f0)\n",
    "        input F0: tensor(batchsize=1, length, dim=1)\n",
    "                  f0 for unvoiced steps should be 0\n",
    "        output sine_tensor: tensor(batchsize=1, length, dim)\n",
    "        output uv: tensor(batchsize=1, length, 1)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            f0 = f0[:, None].transpose(1, 2)\n",
    "            f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim, device=f0.device)\n",
    "            # fundamental component\n",
    "            f0_buf[:, :, 0] = f0[:, :, 0]\n",
    "            for idx in range(self.harmonic_num):\n",
    "                f0_buf[:, :, idx + 1] = f0_buf[:, :, 0] * (\n",
    "                    idx + 2\n",
    "                )  # idx + 2: the (idx+1)-th overtone, (idx+2)-th harmonic\n",
    "            rad_values = (f0_buf / self.sampling_rate) % 1  ###%1意味着n_har的乘积无法后处理优化\n",
    "            rand_ini = torch.rand(\n",
    "                f0_buf.shape[0], f0_buf.shape[2], device=f0_buf.device\n",
    "            )\n",
    "            rand_ini[:, 0] = 0\n",
    "            rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini\n",
    "            tmp_over_one = torch.cumsum(rad_values, 1)  # % 1  #####%1意味着后面的cumsum无法再优化\n",
    "            tmp_over_one *= upp\n",
    "            tmp_over_one = F.interpolate(\n",
    "                tmp_over_one.transpose(2, 1),\n",
    "                scale_factor=float(upp),\n",
    "                mode=\"linear\",\n",
    "                align_corners=True,\n",
    "            ).transpose(2, 1)\n",
    "            rad_values = F.interpolate(\n",
    "                rad_values.transpose(2, 1), scale_factor=float(upp), mode=\"nearest\"\n",
    "            ).transpose(\n",
    "                2, 1\n",
    "            )  #######\n",
    "            tmp_over_one %= 1\n",
    "            tmp_over_one_idx = (tmp_over_one[:, 1:, :] - tmp_over_one[:, :-1, :]) < 0\n",
    "            cumsum_shift = torch.zeros_like(rad_values)\n",
    "            cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0\n",
    "            sine_waves = torch.sin(\n",
    "                torch.cumsum(rad_values + cumsum_shift, dim=1) * 2 * torch.pi\n",
    "            )\n",
    "            sine_waves = sine_waves * self.sine_amp\n",
    "            uv = self._f02uv(f0)\n",
    "            uv = F.interpolate(\n",
    "                uv.transpose(2, 1), scale_factor=float(upp), mode=\"nearest\"\n",
    "            ).transpose(2, 1)\n",
    "            noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3\n",
    "            noise = noise_amp * torch.randn_like(sine_waves)\n",
    "            sine_waves = sine_waves * uv + noise\n",
    "        return sine_waves, uv, noise\n",
    "\n",
    "\n",
    "class SourceModuleHnNSF(torch.nn.Module):\n",
    "    \"\"\"SourceModule for hn-nsf\n",
    "    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n",
    "                 add_noise_std=0.003, voiced_threshod=0)\n",
    "    sampling_rate: sampling_rate in Hz\n",
    "    harmonic_num: number of harmonic above F0 (default: 0)\n",
    "    sine_amp: amplitude of sine source signal (default: 0.1)\n",
    "    add_noise_std: std of additive Gaussian noise (default: 0.003)\n",
    "        note that amplitude of noise in unvoiced is decided\n",
    "        by sine_amp\n",
    "    voiced_threshold: threhold to set U/V given F0 (default: 0)\n",
    "    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n",
    "    F0_sampled (batchsize, length, 1)\n",
    "    Sine_source (batchsize, length, 1)\n",
    "    noise_source (batchsize, length 1)\n",
    "    uv (batchsize, length, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_rate,\n",
    "        harmonic_num=0,\n",
    "        sine_amp=0.1,\n",
    "        add_noise_std=0.003,\n",
    "        voiced_threshod=0,\n",
    "        is_half=True,\n",
    "    ):\n",
    "        super(SourceModuleHnNSF, self).__init__()\n",
    "\n",
    "        self.sine_amp = sine_amp\n",
    "        self.noise_std = add_noise_std\n",
    "        self.is_half = is_half\n",
    "        # to produce sine waveforms\n",
    "        self.l_sin_gen = SineGen(\n",
    "            sampling_rate, harmonic_num, sine_amp, add_noise_std, voiced_threshod\n",
    "        )\n",
    "\n",
    "        # to merge source harmonics into a single excitation\n",
    "        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)\n",
    "        self.l_tanh = torch.nn.Tanh()\n",
    "        # self.ddtype:int = -1\n",
    "\n",
    "    def forward(self, x: torch.Tensor, upp: int = 1):\n",
    "        # if self.ddtype ==-1:\n",
    "        #     self.ddtype = self.l_linear.weight.dtype\n",
    "        sine_wavs, uv, _ = self.l_sin_gen(x, upp)\n",
    "        # print(x.dtype,sine_wavs.dtype,self.l_linear.weight.dtype)\n",
    "        # if self.is_half:\n",
    "        #     sine_wavs = sine_wavs.half()\n",
    "        # sine_merge = self.l_tanh(self.l_linear(sine_wavs.to(x)))\n",
    "        # print(sine_wavs.dtype,self.ddtype)\n",
    "        # if sine_wavs.dtype != self.l_linear.weight.dtype:\n",
    "        sine_wavs = sine_wavs.to(dtype=self.l_linear.weight.dtype)\n",
    "        sine_merge = self.l_tanh(self.l_linear(sine_wavs))\n",
    "        return sine_merge, None, None  # noise, uv\n",
    "\n",
    "\n",
    "class GeneratorNSF(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_channel,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        is_half=False,\n",
    "    ):\n",
    "        super(GeneratorNSF, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "\n",
    "        self.f0_upsamp = torch.nn.Upsample(scale_factor=math.prod(upsample_rates))\n",
    "        self.m_source = SourceModuleHnNSF(\n",
    "            sampling_rate=sr, harmonic_num=0, is_half=is_half\n",
    "        )\n",
    "        self.noise_convs = nn.ModuleList()\n",
    "        self.conv_pre = Conv1d(\n",
    "            initial_channel, upsample_initial_channel, 7, 1, padding=3\n",
    "        )\n",
    "        resblock = ResBlock1 if resblock == \"1\" else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            c_cur = upsample_initial_channel // (2 ** (i + 1))\n",
    "            self.ups.append(\n",
    "                weight_norm(\n",
    "                    ConvTranspose1d(\n",
    "                        upsample_initial_channel // (2**i),\n",
    "                        upsample_initial_channel // (2 ** (i + 1)),\n",
    "                        k,\n",
    "                        u,\n",
    "                        padding=(k - u) // 2,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            if i + 1 < len(upsample_rates):\n",
    "                stride_f0 = math.prod(upsample_rates[i + 1 :])\n",
    "                self.noise_convs.append(\n",
    "                    Conv1d(\n",
    "                        1,\n",
    "                        c_cur,\n",
    "                        kernel_size=stride_f0 * 2,\n",
    "                        stride=stride_f0,\n",
    "                        padding=stride_f0 // 2,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.noise_convs.append(Conv1d(1, c_cur, kernel_size=1))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2 ** (i + 1))\n",
    "            for j, (k, d) in enumerate(\n",
    "                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n",
    "            ):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "        self.upp = math.prod(upsample_rates)\n",
    "\n",
    "        self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "    def forward(self, x, f0, g: Optional[torch.Tensor] = None):\n",
    "        har_source, noi_source, uv = self.m_source(f0, self.upp)\n",
    "        har_source = har_source.transpose(1, 2)\n",
    "        x = self.conv_pre(x)\n",
    "        if g is not None:\n",
    "            x = x + self.cond(g)\n",
    "        # torch.jit.script() does not support direct indexing of torch modules\n",
    "        # That's why I wrote this\n",
    "        for i, (ups, noise_convs) in enumerate(zip(self.ups, self.noise_convs)):\n",
    "            if i < self.num_upsamples:\n",
    "                x = F.leaky_relu(x, self.lrelu_slope)\n",
    "                x = ups(x)\n",
    "                x_source = noise_convs(har_source)\n",
    "                x = x + x_source\n",
    "                xs: Optional[torch.Tensor] = None\n",
    "                l = [i * self.num_kernels + j for j in range(self.num_kernels)]\n",
    "                for j, resblock in enumerate(self.resblocks):\n",
    "                    if j in l:\n",
    "                        if xs is None:\n",
    "                            xs = resblock(x)\n",
    "                        else:\n",
    "                            xs += resblock(x)\n",
    "                # This assertion cannot be ignored! \\\n",
    "                # If ignored, it will cause torch.jit.script() compilation errors\n",
    "                assert isinstance(xs, torch.Tensor)\n",
    "                x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.ups:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.ups:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                # The hook we want to remove is an instance of WeightNorm class, so\n",
    "                # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "                # because of shadowing, so we check the module name directly.\n",
    "                # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            for hook in self.resblocks._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "\n",
    "sr2sr = {\n",
    "    \"32k\": 32000,\n",
    "    \"40k\": 40000,\n",
    "    \"48k\": 48000,\n",
    "}\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs256NSFsid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs256NSFsid, self).__init__()\n",
    "        if isinstance(sr, str):\n",
    "            sr = sr2sr[sr]\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder256(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.dec = GeneratorNSF(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "            sr=sr,\n",
    "            is_half=kwargs[\"is_half\"],\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        pitchf: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        y_lengths: torch.Tensor,\n",
    "        ds: Optional[torch.Tensor] = None,\n",
    "    ):  # 这里ds是id，[bs,1]\n",
    "        # print(1,pitch.shape)#[bs,t]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n",
    "        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n",
    "        # print(-2,pitchf.shape,z_slice.shape)\n",
    "        o = self.dec(z_slice, pitchf, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        nsff0: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            assert isinstance(rate, torch.Tensor)\n",
    "            head = int(z_p.shape[2] * (1 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "            nsff0 = nsff0[:, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, nsff0, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs768NSFsid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs768NSFsid, self).__init__()\n",
    "        if isinstance(sr, str):\n",
    "            sr = sr2sr[sr]\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder768(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.dec = GeneratorNSF(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "            sr=sr,\n",
    "            is_half=kwargs[\"is_half\"],\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(\n",
    "        self, phone, phone_lengths, pitch, pitchf, y, y_lengths, ds\n",
    "    ):  # 这里ds是id，[bs,1]\n",
    "        # print(1,pitch.shape)#[bs,t]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n",
    "        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n",
    "        # print(-2,pitchf.shape,z_slice.shape)\n",
    "        o = self.dec(z_slice, pitchf, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        nsff0: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "            nsff0 = nsff0[:, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, nsff0, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs256NSFsid_nono(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs256NSFsid_nono, self).__init__()\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder256(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "            f0=False,\n",
    "        )\n",
    "        self.dec = Generator(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # 这里ds是id，[bs,1]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        o = self.dec(z_slice, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs768NSFsid_nono(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs768NSFsid_nono, self).__init__()\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder768(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "            f0=False,\n",
    "        )\n",
    "        self.dec = Generator(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # 这里ds是id，[bs,1]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        o = self.dec(z_slice, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(MultiPeriodDiscriminator, self).__init__()\n",
    "        periods = [2, 3, 5, 7, 11, 17]\n",
    "        # periods = [3, 5, 7, 11, 17, 23, 37]\n",
    "\n",
    "        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n",
    "        discs = discs + [\n",
    "            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n",
    "        ]\n",
    "        self.discriminators = nn.ModuleList(discs)\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []  #\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            # for j in range(len(fmap_r)):\n",
    "            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminatorV2(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(MultiPeriodDiscriminatorV2, self).__init__()\n",
    "        # periods = [2, 3, 5, 7, 11, 17]\n",
    "        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n",
    "\n",
    "        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n",
    "        discs = discs + [\n",
    "            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n",
    "        ]\n",
    "        self.discriminators = nn.ModuleList(discs)\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []  #\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            # for j in range(len(fmap_r)):\n",
    "            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "class DiscriminatorS(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(DiscriminatorS, self).__init__()\n",
    "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n",
    "                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n",
    "                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n",
    "                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n",
    "                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n",
    "                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "            ]\n",
    "        )\n",
    "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap\n",
    "\n",
    "\n",
    "class DiscriminatorP(torch.nn.Module):\n",
    "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
    "        super(DiscriminatorP, self).__init__()\n",
    "        self.period = period\n",
    "        self.use_spectral_norm = use_spectral_norm\n",
    "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        1,\n",
    "                        32,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        32,\n",
    "                        128,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        128,\n",
    "                        512,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        512,\n",
    "                        1024,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        1024,\n",
    "                        1024,\n",
    "                        (kernel_size, 1),\n",
    "                        1,\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        # 1d to 2d\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0:  # pad first\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            if has_xpu and x.dtype == torch.bfloat16:\n",
    "                x = F.pad(x.to(dtype=torch.float16), (0, n_pad), \"reflect\").to(\n",
    "                    dtype=torch.bfloat16\n",
    "                )\n",
    "            else:\n",
    "                x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac522336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:45.512658Z",
     "iopub.status.busy": "2025-06-25T01:11:45.512043Z",
     "iopub.status.idle": "2025-06-25T01:11:45.746516Z",
     "shell.execute_reply": "2025-06-25T01:11:45.745455Z"
    },
    "papermill": {
     "duration": 0.294245,
     "end_time": "2025-06-25T01:11:45.748313",
     "exception": false,
     "start_time": "2025-06-25T01:11:45.454068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\n",
    "\n",
    "# from rvc.lib.infer_pack import attentions, commons, modules\n",
    "# from rvc.lib.infer_pack.commons import get_padding, init_weights\n",
    "\n",
    "has_xpu = bool(hasattr(torch, \"xpu\") and torch.xpu.is_available())\n",
    "\n",
    "\n",
    "class TextEncoder256(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        f0=True,\n",
    "    ):\n",
    "        super(TextEncoder256, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.emb_phone = nn.Linear(256, hidden_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n",
    "        if f0 == True:\n",
    "            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n",
    "        self.encoder = Encoder(\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, phone: torch.Tensor, pitch: Optional[torch.Tensor], lengths: torch.Tensor\n",
    "    ):\n",
    "        if pitch is None:\n",
    "            x = self.emb_phone(phone)\n",
    "        else:\n",
    "            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n",
    "        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
    "        x = self.lrelu(x)\n",
    "        x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.encoder(x * x_mask, x_mask)\n",
    "        stats = self.proj(x) * x_mask\n",
    "\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        return m, logs, x_mask\n",
    "\n",
    "\n",
    "class TextEncoder768(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        f0=True,\n",
    "    ):\n",
    "        super(TextEncoder768, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.emb_phone = nn.Linear(768, hidden_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n",
    "        if f0 == True:\n",
    "            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n",
    "        self.encoder = Encoder(\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(self, phone: torch.Tensor, pitch: torch.Tensor, lengths: torch.Tensor):\n",
    "        if pitch is None:\n",
    "            x = self.emb_phone(phone)\n",
    "        else:\n",
    "            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n",
    "        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n",
    "        x = self.lrelu(x)\n",
    "        x = torch.transpose(x, 1, -1)  # [b, h, t]\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.encoder(x * x_mask, x_mask)\n",
    "        stats = self.proj(x) * x_mask\n",
    "\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        return m, logs, x_mask\n",
    "\n",
    "\n",
    "class ResidualCouplingBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        n_flows=4,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(ResidualCouplingBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.n_flows = n_flows\n",
    "        self.gin_channels = gin_channels\n",
    "\n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(n_flows):\n",
    "            self.flows.append(\n",
    "                ResidualCouplingLayer(\n",
    "                    channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size,\n",
    "                    dilation_rate,\n",
    "                    n_layers,\n",
    "                    gin_channels=gin_channels,\n",
    "                    mean_only=True,\n",
    "                )\n",
    "            )\n",
    "            self.flows.append(Flip())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse: bool = False,\n",
    "    ):\n",
    "        if not reverse:\n",
    "            for flow in self.flows:\n",
    "                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n",
    "        else:\n",
    "            for flow in self.flows[::-1]:\n",
    "                x, _ = flow.forward(x, x_mask, g=g, reverse=reverse)\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for i in range(self.n_flows):\n",
    "            self.flows[i * 2].remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for i in range(self.n_flows):\n",
    "            for hook in self.flows[i * 2]._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.flows[i * 2])\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class PosteriorEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(PosteriorEncoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.gin_channels = gin_channels\n",
    "\n",
    "        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n",
    "        self.enc = WN(\n",
    "            hidden_channels,\n",
    "            kernel_size,\n",
    "            dilation_rate,\n",
    "            n_layers,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, x_lengths: torch.Tensor, g: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(\n",
    "            x.dtype\n",
    "        )\n",
    "        x = self.pre(x) * x_mask\n",
    "        x = self.enc(x, x_mask, g=g)\n",
    "        stats = self.proj(x) * x_mask\n",
    "        m, logs = torch.split(stats, self.out_channels, dim=1)\n",
    "        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
    "        return z, m, logs, x_mask\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.enc.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.enc._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.enc)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_channel,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        gin_channels=0,\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = Conv1d(\n",
    "            initial_channel, upsample_initial_channel, 7, 1, padding=3\n",
    "        )\n",
    "        resblock = ResBlock1 if resblock == \"1\" else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(\n",
    "                weight_norm(\n",
    "                    ConvTranspose1d(\n",
    "                        upsample_initial_channel // (2**i),\n",
    "                        upsample_initial_channel // (2 ** (i + 1)),\n",
    "                        k,\n",
    "                        u,\n",
    "                        padding=(k - u) // 2,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2 ** (i + 1))\n",
    "            for j, (k, d) in enumerate(\n",
    "                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n",
    "            ):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, g: Optional[torch.Tensor] = None):\n",
    "        x = self.conv_pre(x)\n",
    "        if g is not None:\n",
    "            x = x + self.cond(g)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i * self.num_kernels + j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i * self.num_kernels + j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.ups:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                # The hook we want to remove is an instance of WeightNorm class, so\n",
    "                # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "                # because of shadowing, so we check the module name directly.\n",
    "                # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "\n",
    "        for l in self.resblocks:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.ups:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "\n",
    "\n",
    "class SineGen(torch.nn.Module):\n",
    "    \"\"\"Definition of sine generator\n",
    "    SineGen(samp_rate, harmonic_num = 0,\n",
    "            sine_amp = 0.1, noise_std = 0.003,\n",
    "            voiced_threshold = 0,\n",
    "            flag_for_pulse=False)\n",
    "    samp_rate: sampling rate in Hz\n",
    "    harmonic_num: number of harmonic overtones (default 0)\n",
    "    sine_amp: amplitude of sine-wavefrom (default 0.1)\n",
    "    noise_std: std of Gaussian noise (default 0.003)\n",
    "    voiced_thoreshold: F0 threshold for U/V classification (default 0)\n",
    "    flag_for_pulse: this SinGen is used inside PulseGen (default False)\n",
    "    Note: when flag_for_pulse is True, the first time step of a voiced\n",
    "        segment is always sin(torch.pi) or cos(0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        samp_rate,\n",
    "        harmonic_num=0,\n",
    "        sine_amp=0.1,\n",
    "        noise_std=0.003,\n",
    "        voiced_threshold=0,\n",
    "        flag_for_pulse=False,\n",
    "    ):\n",
    "        super(SineGen, self).__init__()\n",
    "        self.sine_amp = sine_amp\n",
    "        self.noise_std = noise_std\n",
    "        self.harmonic_num = harmonic_num\n",
    "        self.dim = self.harmonic_num + 1\n",
    "        self.sampling_rate = samp_rate\n",
    "        self.voiced_threshold = voiced_threshold\n",
    "\n",
    "    def _f02uv(self, f0):\n",
    "        # generate uv signal\n",
    "        uv = torch.ones_like(f0)\n",
    "        uv = uv * (f0 > self.voiced_threshold)\n",
    "        if uv.device.type == \"privateuseone\":  # for DirectML\n",
    "            uv = uv.float()\n",
    "        return uv\n",
    "\n",
    "    def forward(self, f0: torch.Tensor, upp: int):\n",
    "        \"\"\"sine_tensor, uv = forward(f0)\n",
    "        input F0: tensor(batchsize=1, length, dim=1)\n",
    "                  f0 for unvoiced steps should be 0\n",
    "        output sine_tensor: tensor(batchsize=1, length, dim)\n",
    "        output uv: tensor(batchsize=1, length, 1)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            f0 = f0[:, None].transpose(1, 2)\n",
    "            f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim, device=f0.device)\n",
    "            # fundamental component\n",
    "            f0_buf[:, :, 0] = f0[:, :, 0]\n",
    "            for idx in range(self.harmonic_num):\n",
    "                f0_buf[:, :, idx + 1] = f0_buf[:, :, 0] * (\n",
    "                    idx + 2\n",
    "                )  # idx + 2: the (idx+1)-th overtone, (idx+2)-th harmonic\n",
    "            rad_values = (f0_buf / self.sampling_rate) % 1  ###%1意味着n_har的乘积无法后处理优化\n",
    "            rand_ini = torch.rand(\n",
    "                f0_buf.shape[0], f0_buf.shape[2], device=f0_buf.device\n",
    "            )\n",
    "            rand_ini[:, 0] = 0\n",
    "            rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini\n",
    "            tmp_over_one = torch.cumsum(rad_values, 1)  # % 1  #####%1意味着后面的cumsum无法再优化\n",
    "            tmp_over_one *= upp\n",
    "            tmp_over_one = F.interpolate(\n",
    "                tmp_over_one.transpose(2, 1),\n",
    "                scale_factor=float(upp),\n",
    "                mode=\"linear\",\n",
    "                align_corners=True,\n",
    "            ).transpose(2, 1)\n",
    "            rad_values = F.interpolate(\n",
    "                rad_values.transpose(2, 1), scale_factor=float(upp), mode=\"nearest\"\n",
    "            ).transpose(\n",
    "                2, 1\n",
    "            )  #######\n",
    "            tmp_over_one %= 1\n",
    "            tmp_over_one_idx = (tmp_over_one[:, 1:, :] - tmp_over_one[:, :-1, :]) < 0\n",
    "            cumsum_shift = torch.zeros_like(rad_values)\n",
    "            cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0\n",
    "            sine_waves = torch.sin(\n",
    "                torch.cumsum(rad_values + cumsum_shift, dim=1) * 2 * torch.pi\n",
    "            )\n",
    "            sine_waves = sine_waves * self.sine_amp\n",
    "            uv = self._f02uv(f0)\n",
    "            uv = F.interpolate(\n",
    "                uv.transpose(2, 1), scale_factor=float(upp), mode=\"nearest\"\n",
    "            ).transpose(2, 1)\n",
    "            noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3\n",
    "            noise = noise_amp * torch.randn_like(sine_waves)\n",
    "            sine_waves = sine_waves * uv + noise\n",
    "        return sine_waves, uv, noise\n",
    "\n",
    "\n",
    "class SourceModuleHnNSF(torch.nn.Module):\n",
    "    \"\"\"SourceModule for hn-nsf\n",
    "    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n",
    "                 add_noise_std=0.003, voiced_threshod=0)\n",
    "    sampling_rate: sampling_rate in Hz\n",
    "    harmonic_num: number of harmonic above F0 (default: 0)\n",
    "    sine_amp: amplitude of sine source signal (default: 0.1)\n",
    "    add_noise_std: std of additive Gaussian noise (default: 0.003)\n",
    "        note that amplitude of noise in unvoiced is decided\n",
    "        by sine_amp\n",
    "    voiced_threshold: threhold to set U/V given F0 (default: 0)\n",
    "    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n",
    "    F0_sampled (batchsize, length, 1)\n",
    "    Sine_source (batchsize, length, 1)\n",
    "    noise_source (batchsize, length 1)\n",
    "    uv (batchsize, length, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_rate,\n",
    "        harmonic_num=0,\n",
    "        sine_amp=0.1,\n",
    "        add_noise_std=0.003,\n",
    "        voiced_threshod=0,\n",
    "        is_half=True,\n",
    "    ):\n",
    "        super(SourceModuleHnNSF, self).__init__()\n",
    "\n",
    "        self.sine_amp = sine_amp\n",
    "        self.noise_std = add_noise_std\n",
    "        self.is_half = is_half\n",
    "        # to produce sine waveforms\n",
    "        self.l_sin_gen = SineGen(\n",
    "            sampling_rate, harmonic_num, sine_amp, add_noise_std, voiced_threshod\n",
    "        )\n",
    "\n",
    "        # to merge source harmonics into a single excitation\n",
    "        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)\n",
    "        self.l_tanh = torch.nn.Tanh()\n",
    "        # self.ddtype:int = -1\n",
    "\n",
    "    def forward(self, x: torch.Tensor, upp: int = 1):\n",
    "        # if self.ddtype ==-1:\n",
    "        #     self.ddtype = self.l_linear.weight.dtype\n",
    "        sine_wavs, uv, _ = self.l_sin_gen(x, upp)\n",
    "        # print(x.dtype,sine_wavs.dtype,self.l_linear.weight.dtype)\n",
    "        # if self.is_half:\n",
    "        #     sine_wavs = sine_wavs.half()\n",
    "        # sine_merge = self.l_tanh(self.l_linear(sine_wavs.to(x)))\n",
    "        # print(sine_wavs.dtype,self.ddtype)\n",
    "        # if sine_wavs.dtype != self.l_linear.weight.dtype:\n",
    "        sine_wavs = sine_wavs.to(dtype=self.l_linear.weight.dtype)\n",
    "        sine_merge = self.l_tanh(self.l_linear(sine_wavs))\n",
    "        return sine_merge, None, None  # noise, uv\n",
    "\n",
    "\n",
    "class GeneratorNSF(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_channel,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        is_half=False,\n",
    "    ):\n",
    "        super(GeneratorNSF, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "\n",
    "        self.f0_upsamp = torch.nn.Upsample(scale_factor=math.prod(upsample_rates))\n",
    "        self.m_source = SourceModuleHnNSF(\n",
    "            sampling_rate=sr, harmonic_num=0, is_half=is_half\n",
    "        )\n",
    "        self.noise_convs = nn.ModuleList()\n",
    "        self.conv_pre = Conv1d(\n",
    "            initial_channel, upsample_initial_channel, 7, 1, padding=3\n",
    "        )\n",
    "        resblock = ResBlock1 if resblock == \"1\" else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            c_cur = upsample_initial_channel // (2 ** (i + 1))\n",
    "            self.ups.append(\n",
    "                weight_norm(\n",
    "                    ConvTranspose1d(\n",
    "                        upsample_initial_channel // (2**i),\n",
    "                        upsample_initial_channel // (2 ** (i + 1)),\n",
    "                        k,\n",
    "                        u,\n",
    "                        padding=(k - u) // 2,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            if i + 1 < len(upsample_rates):\n",
    "                stride_f0 = math.prod(upsample_rates[i + 1 :])\n",
    "                self.noise_convs.append(\n",
    "                    Conv1d(\n",
    "                        1,\n",
    "                        c_cur,\n",
    "                        kernel_size=stride_f0 * 2,\n",
    "                        stride=stride_f0,\n",
    "                        padding=stride_f0 // 2,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.noise_convs.append(Conv1d(1, c_cur, kernel_size=1))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2 ** (i + 1))\n",
    "            for j, (k, d) in enumerate(\n",
    "                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n",
    "            ):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
    "        self.ups.apply(init_weights)\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
    "\n",
    "        self.upp = math.prod(upsample_rates)\n",
    "\n",
    "        self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "    def forward(self, x, f0, g: Optional[torch.Tensor] = None):\n",
    "        har_source, noi_source, uv = self.m_source(f0, self.upp)\n",
    "        har_source = har_source.transpose(1, 2)\n",
    "        x = self.conv_pre(x)\n",
    "        if g is not None:\n",
    "            x = x + self.cond(g)\n",
    "        # torch.jit.script() does not support direct indexing of torch modules\n",
    "        # That's why I wrote this\n",
    "        for i, (ups, noise_convs) in enumerate(zip(self.ups, self.noise_convs)):\n",
    "            if i < self.num_upsamples:\n",
    "                x = F.leaky_relu(x, self.lrelu_slope)\n",
    "                x = ups(x)\n",
    "                x_source = noise_convs(har_source)\n",
    "                x = x + x_source\n",
    "                xs: Optional[torch.Tensor] = None\n",
    "                l = [i * self.num_kernels + j for j in range(self.num_kernels)]\n",
    "                for j, resblock in enumerate(self.resblocks):\n",
    "                    if j in l:\n",
    "                        if xs is None:\n",
    "                            xs = resblock(x)\n",
    "                        else:\n",
    "                            xs += resblock(x)\n",
    "                # This assertion cannot be ignored! \\\n",
    "                # If ignored, it will cause torch.jit.script() compilation errors\n",
    "                assert isinstance(xs, torch.Tensor)\n",
    "                x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.ups:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.ups:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                # The hook we want to remove is an instance of WeightNorm class, so\n",
    "                # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "                # because of shadowing, so we check the module name directly.\n",
    "                # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            for hook in self.resblocks._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "\n",
    "sr2sr = {\n",
    "    \"32k\": 32000,\n",
    "    \"40k\": 40000,\n",
    "    \"48k\": 48000,\n",
    "}\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs256NSFsid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs256NSFsid, self).__init__()\n",
    "        if isinstance(sr, str):\n",
    "            sr = sr2sr[sr]\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder256(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.dec = GeneratorNSF(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "            sr=sr,\n",
    "            is_half=kwargs[\"is_half\"],\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        pitchf: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        y_lengths: torch.Tensor,\n",
    "        ds: Optional[torch.Tensor] = None,\n",
    "    ):  # 这里ds是id，[bs,1]\n",
    "        # print(1,pitch.shape)#[bs,t]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n",
    "        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n",
    "        # print(-2,pitchf.shape,z_slice.shape)\n",
    "        o = self.dec(z_slice, pitchf, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        nsff0: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            assert isinstance(rate, torch.Tensor)\n",
    "            head = int(z_p.shape[2] * (1 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "            nsff0 = nsff0[:, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, nsff0, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs768NSFsid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs768NSFsid, self).__init__()\n",
    "        if isinstance(sr, str):\n",
    "            sr = sr2sr[sr]\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder768(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "        )\n",
    "        self.dec = GeneratorNSF(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "            sr=sr,\n",
    "            is_half=kwargs[\"is_half\"],\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(\n",
    "        self, phone, phone_lengths, pitch, pitchf, y, y_lengths, ds\n",
    "    ):  # 这里ds是id，[bs,1]\n",
    "        # print(1,pitch.shape)#[bs,t]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n",
    "        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n",
    "        # print(-2,pitchf.shape,z_slice.shape)\n",
    "        o = self.dec(z_slice, pitchf, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        pitch: torch.Tensor,\n",
    "        nsff0: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "            nsff0 = nsff0[:, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, nsff0, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs256NSFsid_nono(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs256NSFsid_nono, self).__init__()\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder256(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "            f0=False,\n",
    "        )\n",
    "        self.dec = Generator(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # 这里ds是id，[bs,1]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        o = self.dec(z_slice, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class SynthesizerTrnMs768NSFsid_nono(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_channels,\n",
    "        segment_size,\n",
    "        inter_channels,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        resblock,\n",
    "        resblock_kernel_sizes,\n",
    "        resblock_dilation_sizes,\n",
    "        upsample_rates,\n",
    "        upsample_initial_channel,\n",
    "        upsample_kernel_sizes,\n",
    "        spk_embed_dim,\n",
    "        gin_channels,\n",
    "        sr=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SynthesizerTrnMs768NSFsid_nono, self).__init__()\n",
    "        self.spec_channels = spec_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        self.resblock = resblock\n",
    "        self.resblock_kernel_sizes = resblock_kernel_sizes\n",
    "        self.resblock_dilation_sizes = resblock_dilation_sizes\n",
    "        self.upsample_rates = upsample_rates\n",
    "        self.upsample_initial_channel = upsample_initial_channel\n",
    "        self.upsample_kernel_sizes = upsample_kernel_sizes\n",
    "        self.segment_size = segment_size\n",
    "        self.gin_channels = gin_channels\n",
    "        # self.hop_length = hop_length#\n",
    "        self.spk_embed_dim = spk_embed_dim\n",
    "        self.enc_p = TextEncoder768(\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            float(p_dropout),\n",
    "            f0=False,\n",
    "        )\n",
    "        self.dec = Generator(\n",
    "            inter_channels,\n",
    "            resblock,\n",
    "            resblock_kernel_sizes,\n",
    "            resblock_dilation_sizes,\n",
    "            upsample_rates,\n",
    "            upsample_initial_channel,\n",
    "            upsample_kernel_sizes,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.enc_q = PosteriorEncoder(\n",
    "            spec_channels,\n",
    "            inter_channels,\n",
    "            hidden_channels,\n",
    "            5,\n",
    "            1,\n",
    "            16,\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.flow = ResidualCouplingBlock(\n",
    "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n",
    "        )\n",
    "        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n",
    "        logger.debug(\n",
    "            \"gin_channels: \"\n",
    "            + str(gin_channels)\n",
    "            + \", self.spk_embed_dim: \"\n",
    "            + str(self.spk_embed_dim)\n",
    "        )\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.dec.remove_weight_norm()\n",
    "        self.flow.remove_weight_norm()\n",
    "        self.enc_q.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.dec._forward_pre_hooks.values():\n",
    "            # The hook we want to remove is an instance of WeightNorm class, so\n",
    "            # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "            # because of shadowing, so we check the module name directly.\n",
    "            # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.dec)\n",
    "        for hook in self.flow._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.flow)\n",
    "        if hasattr(self, \"enc_q\"):\n",
    "            for hook in self.enc_q._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.enc_q)\n",
    "        return self\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # 这里ds是id，[bs,1]\n",
    "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "        z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            z, y_lengths, self.segment_size\n",
    "        )\n",
    "        o = self.dec(z_slice, g=g)\n",
    "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self,\n",
    "        phone: torch.Tensor,\n",
    "        phone_lengths: torch.Tensor,\n",
    "        sid: torch.Tensor,\n",
    "        rate: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        g = self.emb_g(sid).unsqueeze(-1)\n",
    "        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n",
    "        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n",
    "        if rate is not None:\n",
    "            head = int(z_p.shape[2] * (1.0 - rate.item()))\n",
    "            z_p = z_p[:, :, head:]\n",
    "            x_mask = x_mask[:, :, head:]\n",
    "        z = self.flow(z_p, x_mask, g=g, reverse=True)\n",
    "        o = self.dec(z * x_mask, g=g)\n",
    "        return o, x_mask, (z, z_p, m_p, logs_p)\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(MultiPeriodDiscriminator, self).__init__()\n",
    "        periods = [2, 3, 5, 7, 11, 17]\n",
    "        # periods = [3, 5, 7, 11, 17, 23, 37]\n",
    "\n",
    "        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n",
    "        discs = discs + [\n",
    "            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n",
    "        ]\n",
    "        self.discriminators = nn.ModuleList(discs)\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []  #\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            # for j in range(len(fmap_r)):\n",
    "            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminatorV2(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(MultiPeriodDiscriminatorV2, self).__init__()\n",
    "        # periods = [2, 3, 5, 7, 11, 17]\n",
    "        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n",
    "\n",
    "        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n",
    "        discs = discs + [\n",
    "            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n",
    "        ]\n",
    "        self.discriminators = nn.ModuleList(discs)\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []  #\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            # for j in range(len(fmap_r)):\n",
    "            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "class DiscriminatorS(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(DiscriminatorS, self).__init__()\n",
    "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n",
    "                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n",
    "                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n",
    "                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n",
    "                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n",
    "                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "            ]\n",
    "        )\n",
    "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap\n",
    "\n",
    "\n",
    "class DiscriminatorP(torch.nn.Module):\n",
    "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
    "        super(DiscriminatorP, self).__init__()\n",
    "        self.period = period\n",
    "        self.use_spectral_norm = use_spectral_norm\n",
    "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        1,\n",
    "                        32,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        32,\n",
    "                        128,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        128,\n",
    "                        512,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        512,\n",
    "                        1024,\n",
    "                        (kernel_size, 1),\n",
    "                        (stride, 1),\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "                norm_f(\n",
    "                    Conv2d(\n",
    "                        1024,\n",
    "                        1024,\n",
    "                        (kernel_size, 1),\n",
    "                        1,\n",
    "                        padding=(get_padding(kernel_size, 1), 0),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        # 1d to 2d\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0:  # pad first\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            if has_xpu and x.dtype == torch.bfloat16:\n",
    "                x = F.pad(x.to(dtype=torch.float16), (0, n_pad), \"reflect\").to(\n",
    "                    dtype=torch.bfloat16\n",
    "                )\n",
    "            else:\n",
    "                x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074db651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:11:45.845794Z",
     "iopub.status.busy": "2025-06-25T01:11:45.844961Z",
     "iopub.status.idle": "2025-06-25T01:12:23.903680Z",
     "shell.execute_reply": "2025-06-25T01:12:23.902320Z"
    },
    "papermill": {
     "duration": 38.107416,
     "end_time": "2025-06-25T01:12:23.905509",
     "exception": false,
     "start_time": "2025-06-25T01:11:45.798093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyworld\r\n",
      "  Downloading pyworld-0.3.5.tar.gz (261 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyworld) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyworld) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyworld) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pyworld) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pyworld) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pyworld) (2024.2.0)\r\n",
      "Building wheels for collected packages: pyworld\r\n",
      "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pyworld: filename=pyworld-0.3.5-cp311-cp311-linux_x86_64.whl size=925758 sha256=5b8328870b1501275f191bf0c45e442387d99ae8986ade109d35eb4dc31381f6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/f0/db/ebcd5cdfe5ad7d229917d3a8db6f18f0cf40f099bf878e294d\r\n",
      "Successfully built pyworld\r\n",
      "Installing collected packages: pyworld\r\n",
      "Successfully installed pyworld-0.3.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee4a621f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:12:23.990452Z",
     "iopub.status.busy": "2025-06-25T01:12:23.989598Z",
     "iopub.status.idle": "2025-06-25T01:12:34.987370Z",
     "shell.execute_reply": "2025-06-25T01:12:34.986141Z"
    },
    "papermill": {
     "duration": 11.042048,
     "end_time": "2025-06-25T01:12:34.989177",
     "exception": false,
     "start_time": "2025-06-25T01:12:23.947129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\r\n",
      "Collecting resampy\r\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting torchcrepe\r\n",
      "  Downloading torchcrepe-0.0.24-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\r\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (1.26.4)\r\n",
      "Requirement already satisfied: librosa>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (0.11.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (1.15.2)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (4.67.1)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (3.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.5.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (4.4.2)\r\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.13.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.8.2)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.5.0.post1)\r\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (4.13.2)\r\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1,>=1.22->numba) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.18.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchcrepe) (1.3.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa>=0.9.1->torchcrepe) (25.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->torchcrepe) (4.3.8)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->torchcrepe) (2.32.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.9.1->torchcrepe) (3.6.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa>=0.9.1->torchcrepe) (1.17.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchcrepe) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1,>=1.22->numba) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1,>=1.22->numba) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.9.1->torchcrepe) (2.22)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (2025.4.26)\r\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torchcrepe-0.0.24-py3-none-any.whl (72.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: resampy, torchcrepe\r\n",
      "Successfully installed resampy-0.4.3 torchcrepe-0.0.24\r\n",
      "Collecting praat-parselmouth\r\n",
      "  Downloading praat_parselmouth-0.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from praat-parselmouth) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7.0->praat-parselmouth) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7.0->praat-parselmouth) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7.0->praat-parselmouth) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7.0->praat-parselmouth) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7.0->praat-parselmouth) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7.0->praat-parselmouth) (2024.2.0)\r\n",
      "Downloading praat_parselmouth-0.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: praat-parselmouth\r\n",
      "Successfully installed praat-parselmouth-0.4.6\r\n"
     ]
    }
   ],
   "source": [
    "# First uninstall any problematic installations\n",
    "# !pip uninstall -y googleads parselmouth\n",
    "\n",
    "# # Then install Parselmouth with --no-deps to avoid problematic dependencies\n",
    "# !pip install parselmouth --no-deps\n",
    "\n",
    "# # Install numpy first with correct version\n",
    "# !pip install \"numpy<2.0.0\"\n",
    "\n",
    "# # Then install other required packages\n",
    "!pip install numba resampy torchcrepe\n",
    "\n",
    "# First remove any existing installation\n",
    "# !pip uninstall -y parselmouth\n",
    "\n",
    "# Install the correct version that supports Python 3.11\n",
    "!pip install praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6466cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:12:35.082469Z",
     "iopub.status.busy": "2025-06-25T01:12:35.082100Z",
     "iopub.status.idle": "2025-06-25T01:12:52.678356Z",
     "shell.execute_reply": "2025-06-25T01:12:52.676963Z"
    },
    "papermill": {
     "duration": 17.646168,
     "end_time": "2025-06-25T01:12:52.680302",
     "exception": false,
     "start_time": "2025-06-25T01:12:35.034134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.15.2\r\n",
      "Uninstalling scipy-1.15.2:\r\n",
      "  Successfully uninstalled scipy-1.15.2\r\n",
      "Found existing installation: numpy 1.26.4\r\n",
      "Uninstalling numpy-1.26.4:\r\n",
      "  Successfully uninstalled numpy-1.26.4\r\n",
      "Collecting scipy==1.10.1\r\n",
      "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy==1.24.3\r\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\r\n",
      "Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "faiss-cpu 1.11.0 requires numpy<3.0,>=1.25.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\r\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\r\n",
      "woodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "featuretools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "kaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "bayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\r\n",
      "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\r\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\r\n",
      "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\r\n",
      "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 scipy-1.10.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y scipy numpy\n",
    "!pip install scipy==1.10.1 numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f46a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T01:12:52.775698Z",
     "iopub.status.busy": "2025-06-25T01:12:52.775353Z",
     "iopub.status.idle": "2025-06-25T01:12:53.032748Z",
     "shell.execute_reply": "2025-06-25T01:12:53.030373Z"
    },
    "papermill": {
     "duration": 0.307202,
     "end_time": "2025-06-25T01:12:53.034787",
     "exception": true,
     "start_time": "2025-06-25T01:12:52.727585",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/METADATA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   3184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3395\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3397\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   3184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _pkg_info",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/1293229391.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparselmouth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyworld/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyworld'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpyworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected str, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0mnew_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m             \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   3106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRequirement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3107\u001b[0m         \u001b[0;34m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3108\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3109\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRequirement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0mreqs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRequirement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m         \u001b[0;31m# Including any condition expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3416\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parsed_pkg_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Requires-Dist'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3417\u001b[0m             \u001b[0mreqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPKG_INFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1699\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/METADATA'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from functools import lru_cache\n",
    "from time import time as ttime\n",
    "\n",
    "import faiss\n",
    "import librosa\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import pyworld\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchcrepe\n",
    "import scipy._lib._util\n",
    "if not hasattr(scipy._lib._util, 'prod'):\n",
    "    scipy._lib._util.prod = lambda x: x.prod()\n",
    "    \n",
    "from scipy import signal\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n",
    "\n",
    "input_audio_path2wav = {}\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def cache_harvest_f0(input_audio_path, fs, f0max, f0min, frame_period):\n",
    "    audio = input_audio_path2wav[input_audio_path]\n",
    "    f0, t = pyworld.harvest(\n",
    "        audio,\n",
    "        fs=fs,\n",
    "        f0_ceil=f0max,\n",
    "        f0_floor=f0min,\n",
    "        frame_period=frame_period,\n",
    "    )\n",
    "    f0 = pyworld.stonemask(audio, f0, t, fs)\n",
    "    return f0\n",
    "\n",
    "\n",
    "def change_rms(data1, sr1, data2, sr2, rate):  # 1是输入音频，2是输出音频,rate是2的占比\n",
    "    # print(data1.max(),data2.max())\n",
    "    rms1 = librosa.feature.rms(\n",
    "        y=data1, frame_length=sr1 // 2 * 2, hop_length=sr1 // 2\n",
    "    )  # 每半秒一个点\n",
    "    rms2 = librosa.feature.rms(y=data2, frame_length=sr2 // 2 * 2, hop_length=sr2 // 2)\n",
    "    rms1 = torch.from_numpy(rms1)\n",
    "    rms1 = F.interpolate(\n",
    "        rms1.unsqueeze(0), size=data2.shape[0], mode=\"linear\"\n",
    "    ).squeeze()\n",
    "    rms2 = torch.from_numpy(rms2)\n",
    "    rms2 = F.interpolate(\n",
    "        rms2.unsqueeze(0), size=data2.shape[0], mode=\"linear\"\n",
    "    ).squeeze()\n",
    "    rms2 = torch.max(rms2, torch.zeros_like(rms2) + 1e-6)\n",
    "    data2 *= (\n",
    "        torch.pow(rms1, torch.tensor(1 - rate))\n",
    "        * torch.pow(rms2, torch.tensor(rate - 1))\n",
    "    ).numpy()\n",
    "    return data2\n",
    "\n",
    "\n",
    "class Pipeline(object):\n",
    "    def __init__(self, tgt_sr, config):\n",
    "        self.x_pad, self.x_query, self.x_center, self.x_max, self.is_half = (\n",
    "            config.x_pad,\n",
    "            config.x_query,\n",
    "            config.x_center,\n",
    "            config.x_max,\n",
    "            config.is_half,\n",
    "        )\n",
    "        self.sr = 16000  # hubert输入采样率\n",
    "        self.window = 160  # 每帧点数\n",
    "        self.t_pad = self.sr * self.x_pad  # 每条前后pad时间\n",
    "        self.t_pad_tgt = tgt_sr * self.x_pad\n",
    "        self.t_pad2 = self.t_pad * 2\n",
    "        self.t_query = self.sr * self.x_query  # 查询切点前后查询时间\n",
    "        self.t_center = self.sr * self.x_center  # 查询切点位置\n",
    "        self.t_max = self.sr * self.x_max  # 免查询时长阈值\n",
    "        self.device = config.device\n",
    "\n",
    "    def get_f0(\n",
    "        self,\n",
    "        input_audio_path,\n",
    "        x,\n",
    "        p_len,\n",
    "        f0_up_key,\n",
    "        f0_method,\n",
    "        filter_radius,\n",
    "        inp_f0=None,\n",
    "    ):\n",
    "        global input_audio_path2wav\n",
    "        time_step = self.window / self.sr * 1000\n",
    "        f0_min = 50\n",
    "        f0_max = 1100\n",
    "        f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n",
    "        f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n",
    "        if f0_method == \"pm\":\n",
    "            f0 = (\n",
    "                parselmouth.Sound(x, self.sr)\n",
    "                .to_pitch_ac(\n",
    "                    time_step=time_step / 1000,\n",
    "                    voicing_threshold=0.6,\n",
    "                    pitch_floor=f0_min,\n",
    "                    pitch_ceiling=f0_max,\n",
    "                )\n",
    "                .selected_array[\"frequency\"]\n",
    "            )\n",
    "            pad_size = (p_len - len(f0) + 1) // 2\n",
    "            if pad_size > 0 or p_len - len(f0) - pad_size > 0:\n",
    "                f0 = np.pad(\n",
    "                    f0, [[pad_size, p_len - len(f0) - pad_size]], mode=\"constant\"\n",
    "                )\n",
    "        elif f0_method == \"harvest\":\n",
    "            input_audio_path2wav[input_audio_path] = x.astype(np.double)\n",
    "            f0 = cache_harvest_f0(input_audio_path, self.sr, f0_max, f0_min, 10)\n",
    "            if filter_radius > 2:\n",
    "                f0 = signal.medfilt(f0, 3)\n",
    "        elif f0_method == \"crepe\":\n",
    "            model = \"full\"\n",
    "            # Pick a batch size that doesn't cause memory errors on your gpu\n",
    "            batch_size = 512\n",
    "            # Compute pitch using first gpu\n",
    "            audio = torch.tensor(np.copy(x))[None].float()\n",
    "            f0, pd = torchcrepe.predict(\n",
    "                audio,\n",
    "                self.sr,\n",
    "                self.window,\n",
    "                f0_min,\n",
    "                f0_max,\n",
    "                model,\n",
    "                batch_size=batch_size,\n",
    "                device=self.device,\n",
    "                return_periodicity=True,\n",
    "            )\n",
    "            pd = torchcrepe.filter.median(pd, 3)\n",
    "            f0 = torchcrepe.filter.mean(f0, 3)\n",
    "            f0[pd < 0.1] = 0\n",
    "            f0 = f0[0].cpu().numpy()\n",
    "        elif f0_method == \"rmvpe\":\n",
    "            if not hasattr(self, \"model_rmvpe\"):\n",
    "                from rvc.lib.rmvpe import RMVPE\n",
    "\n",
    "                logger.info(\n",
    "                    \"Loading rmvpe model,%s\" % \"%s/rmvpe.pt\" % os.environ[\"rmvpe_root\"]\n",
    "                )\n",
    "                self.model_rmvpe = RMVPE(\n",
    "                    \"%s/rmvpe.pt\" % os.environ[\"rmvpe_root\"],\n",
    "                    is_half=self.is_half,\n",
    "                    device=self.device,\n",
    "                )\n",
    "            f0 = self.model_rmvpe.infer_from_audio(x, thred=0.03)\n",
    "\n",
    "            if \"privateuseone\" in str(self.device):  # clean ortruntime memory\n",
    "                del self.model_rmvpe.model\n",
    "                del self.model_rmvpe\n",
    "                logger.info(\"Cleaning ortruntime memory\")\n",
    "\n",
    "        f0 *= pow(2, f0_up_key / 12)\n",
    "        # with open(\"test.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))\n",
    "        tf0 = self.sr // self.window  # 每秒f0点数\n",
    "        if inp_f0 is not None:\n",
    "            delta_t = np.round(\n",
    "                (inp_f0[:, 0].max() - inp_f0[:, 0].min()) * tf0 + 1\n",
    "            ).astype(\"int16\")\n",
    "            replace_f0 = np.interp(\n",
    "                list(range(delta_t)), inp_f0[:, 0] * 100, inp_f0[:, 1]\n",
    "            )\n",
    "            shape = f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)].shape[0]\n",
    "            f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)] = replace_f0[\n",
    "                :shape\n",
    "            ]\n",
    "        # with open(\"test_opt.txt\",\"w\")as f:f.write(\"\\n\".join([str(i)for i in f0.tolist()]))\n",
    "        f0bak = f0.copy()\n",
    "        f0_mel = 1127 * np.log(1 + f0 / 700)\n",
    "        f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n",
    "            f0_mel_max - f0_mel_min\n",
    "        ) + 1\n",
    "        f0_mel[f0_mel <= 1] = 1\n",
    "        f0_mel[f0_mel > 255] = 255\n",
    "        f0_coarse = np.rint(f0_mel).astype(np.int32)\n",
    "        return f0_coarse, f0bak  # 1-0\n",
    "\n",
    "    def vc(\n",
    "        self,\n",
    "        model,\n",
    "        net_g,\n",
    "        sid,\n",
    "        audio0,\n",
    "        pitch,\n",
    "        pitchf,\n",
    "        times,\n",
    "        index,\n",
    "        big_npy,\n",
    "        index_rate,\n",
    "        version,\n",
    "        protect,\n",
    "    ):  # ,file_index,file_big_npy\n",
    "        feats = torch.from_numpy(audio0)\n",
    "        if self.is_half:\n",
    "            feats = feats.half()\n",
    "        else:\n",
    "            feats = feats.float()\n",
    "        if feats.dim() == 2:  # double channels\n",
    "            feats = feats.mean(-1)\n",
    "        assert feats.dim() == 1, feats.dim()\n",
    "        feats = feats.view(1, -1)\n",
    "        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n",
    "\n",
    "        inputs = {\n",
    "            \"source\": feats.to(self.device),\n",
    "            \"padding_mask\": padding_mask,\n",
    "            \"output_layer\": 9 if version == \"v1\" else 12,\n",
    "        }\n",
    "        t0 = ttime()\n",
    "        with torch.no_grad():\n",
    "            logits = model.extract_features(**inputs)\n",
    "            feats = model.final_proj(logits[0]) if version == \"v1\" else logits[0]\n",
    "        if protect < 0.5 and pitch is not None and pitchf is not None:\n",
    "            feats0 = feats.clone()\n",
    "        if (\n",
    "            not isinstance(index, type(None))\n",
    "            and not isinstance(big_npy, type(None))\n",
    "            and index_rate != 0\n",
    "        ):\n",
    "            npy = feats[0].cpu().numpy()\n",
    "            if self.is_half:\n",
    "                npy = npy.astype(\"float32\")\n",
    "\n",
    "            # _, I = index.search(npy, 1)\n",
    "            # npy = big_npy[I.squeeze()]\n",
    "\n",
    "            score, ix = index.search(npy, k=8)\n",
    "            weight = np.square(1 / score)\n",
    "            weight /= weight.sum(axis=1, keepdims=True)\n",
    "            npy = np.sum(big_npy[ix] * np.expand_dims(weight, axis=2), axis=1)\n",
    "\n",
    "            if self.is_half:\n",
    "                npy = npy.astype(\"float16\")\n",
    "            feats = (\n",
    "                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n",
    "                + (1 - index_rate) * feats\n",
    "            )\n",
    "\n",
    "        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n",
    "        if protect < 0.5 and pitch is not None and pitchf is not None:\n",
    "            feats0 = F.interpolate(feats0.permute(0, 2, 1), scale_factor=2).permute(\n",
    "                0, 2, 1\n",
    "            )\n",
    "        t1 = ttime()\n",
    "        p_len = audio0.shape[0] // self.window\n",
    "        if feats.shape[1] < p_len:\n",
    "            p_len = feats.shape[1]\n",
    "            if pitch is not None and pitchf is not None:\n",
    "                pitch = pitch[:, :p_len]\n",
    "                pitchf = pitchf[:, :p_len]\n",
    "\n",
    "        if protect < 0.5 and pitch is not None and pitchf is not None:\n",
    "            pitchff = pitchf.clone()\n",
    "            pitchff[pitchf > 0] = 1\n",
    "            pitchff[pitchf < 1] = protect\n",
    "            pitchff = pitchff.unsqueeze(-1)\n",
    "            feats = feats * pitchff + feats0 * (1 - pitchff)\n",
    "            feats = feats.to(feats0.dtype)\n",
    "        p_len = torch.tensor([p_len], device=self.device).long()\n",
    "        with torch.no_grad():\n",
    "            hasp = pitch is not None and pitchf is not None\n",
    "            arg = (feats, p_len, pitch, pitchf, sid) if hasp else (feats, p_len, sid)\n",
    "            audio1 = (net_g.infer(*arg)[0][0, 0]).data.cpu().float().numpy()\n",
    "            del hasp, arg\n",
    "        del feats, p_len, padding_mask\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        t2 = ttime()\n",
    "        times[\"npy\"] += t1 - t0\n",
    "        times[\"infer\"] += t2 - t1\n",
    "        return audio1\n",
    "\n",
    "    def pipeline(\n",
    "        self,\n",
    "        model,\n",
    "        net_g,\n",
    "        sid,\n",
    "        audio,\n",
    "        input_audio_path,\n",
    "        times,\n",
    "        f0_up_key,\n",
    "        f0_method,\n",
    "        file_index,\n",
    "        index_rate,\n",
    "        if_f0,\n",
    "        filter_radius,\n",
    "        tgt_sr,\n",
    "        resample_sr,\n",
    "        rms_mix_rate,\n",
    "        version,\n",
    "        protect,\n",
    "        f0_file=None,\n",
    "    ):\n",
    "        if (\n",
    "            file_index\n",
    "            and file_index != \"\"\n",
    "            # and file_big_npy != \"\"\n",
    "            # and os.path.exists(file_big_npy) == True\n",
    "            and os.path.exists(file_index)\n",
    "            and index_rate != 0\n",
    "        ):\n",
    "            try:\n",
    "                index = faiss.read_index(file_index)\n",
    "                # big_npy = np.load(file_big_npy)\n",
    "                big_npy = index.reconstruct_n(0, index.ntotal)\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                index = big_npy = None\n",
    "        else:\n",
    "            index = big_npy = None\n",
    "        audio = signal.filtfilt(bh, ah, audio)\n",
    "        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\")\n",
    "        opt_ts = []\n",
    "        if audio_pad.shape[0] > self.t_max:\n",
    "            audio_sum = np.zeros_like(audio)\n",
    "            for i in range(self.window):\n",
    "                audio_sum += np.abs(audio_pad[i : i - self.window])\n",
    "            for t in range(self.t_center, audio.shape[0], self.t_center):\n",
    "                opt_ts.append(\n",
    "                    t\n",
    "                    - self.t_query\n",
    "                    + np.where(\n",
    "                        audio_sum[t - self.t_query : t + self.t_query]\n",
    "                        == audio_sum[t - self.t_query : t + self.t_query].min()\n",
    "                    )[0][0]\n",
    "                )\n",
    "        s = 0\n",
    "        audio_opt = []\n",
    "        t = None\n",
    "        t1 = ttime()\n",
    "        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\")\n",
    "        p_len = audio_pad.shape[0] // self.window\n",
    "        inp_f0 = None\n",
    "        if hasattr(f0_file, \"name\"):\n",
    "            try:\n",
    "                with open(f0_file.name, \"r\") as f:\n",
    "                    lines = f.read().strip(\"\\n\").split(\"\\n\")\n",
    "                inp_f0 = []\n",
    "                for line in lines:\n",
    "                    inp_f0.append([float(i) for i in line.split(\",\")])\n",
    "                inp_f0 = np.array(inp_f0, dtype=\"float32\")\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n",
    "        pitch, pitchf = None, None\n",
    "        if if_f0 == 1:\n",
    "            pitch, pitchf = self.get_f0(\n",
    "                input_audio_path,\n",
    "                audio_pad,\n",
    "                p_len,\n",
    "                f0_up_key,\n",
    "                f0_method,\n",
    "                filter_radius,\n",
    "                inp_f0,\n",
    "            )\n",
    "            pitch = pitch[:p_len]\n",
    "            pitchf = pitchf[:p_len]\n",
    "            if \"mps\" not in str(self.device) or \"xpu\" not in str(self.device):\n",
    "                pitchf = pitchf.astype(np.float32)\n",
    "            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n",
    "            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n",
    "        t2 = ttime()\n",
    "        times[\"f0\"] += t2 - t1\n",
    "        for t in opt_ts:\n",
    "            t = t // self.window * self.window\n",
    "            if if_f0 == 1:\n",
    "                audio_opt.append(\n",
    "                    self.vc(\n",
    "                        model,\n",
    "                        net_g,\n",
    "                        sid,\n",
    "                        audio_pad[s : t + self.t_pad2 + self.window],\n",
    "                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n",
    "                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n",
    "                        times,\n",
    "                        index,\n",
    "                        big_npy,\n",
    "                        index_rate,\n",
    "                        version,\n",
    "                        protect,\n",
    "                    )[self.t_pad_tgt : -self.t_pad_tgt]\n",
    "                )\n",
    "            else:\n",
    "                audio_opt.append(\n",
    "                    self.vc(\n",
    "                        model,\n",
    "                        net_g,\n",
    "                        sid,\n",
    "                        audio_pad[s : t + self.t_pad2 + self.window],\n",
    "                        None,\n",
    "                        None,\n",
    "                        times,\n",
    "                        index,\n",
    "                        big_npy,\n",
    "                        index_rate,\n",
    "                        version,\n",
    "                        protect,\n",
    "                    )[self.t_pad_tgt : -self.t_pad_tgt]\n",
    "                )\n",
    "            s = t\n",
    "        if if_f0 == 1:\n",
    "            audio_opt.append(\n",
    "                self.vc(\n",
    "                    model,\n",
    "                    net_g,\n",
    "                    sid,\n",
    "                    audio_pad[t:],\n",
    "                    pitch[:, t // self.window :] if t is not None else pitch,\n",
    "                    pitchf[:, t // self.window :] if t is not None else pitchf,\n",
    "                    times,\n",
    "                    index,\n",
    "                    big_npy,\n",
    "                    index_rate,\n",
    "                    version,\n",
    "                    protect,\n",
    "                )[self.t_pad_tgt : -self.t_pad_tgt]\n",
    "            )\n",
    "        else:\n",
    "            audio_opt.append(\n",
    "                self.vc(\n",
    "                    model,\n",
    "                    net_g,\n",
    "                    sid,\n",
    "                    audio_pad[t:],\n",
    "                    None,\n",
    "                    None,\n",
    "                    times,\n",
    "                    index,\n",
    "                    big_npy,\n",
    "                    index_rate,\n",
    "                    version,\n",
    "                    protect,\n",
    "                )[self.t_pad_tgt : -self.t_pad_tgt]\n",
    "            )\n",
    "        audio_opt = np.concatenate(audio_opt)\n",
    "        if rms_mix_rate != 1:\n",
    "            audio_opt = change_rms(audio, 16000, audio_opt, tgt_sr, rms_mix_rate)\n",
    "        if tgt_sr != resample_sr >= 16000:\n",
    "            audio_opt = librosa.resample(\n",
    "                audio_opt, orig_sr=tgt_sr, target_sr=resample_sr\n",
    "            )\n",
    "        audio_max = np.abs(audio_opt).max() / 0.99\n",
    "        max_int16 = 32768\n",
    "        if audio_max > 1:\n",
    "            max_int16 /= audio_max\n",
    "        audio_opt = (audio_opt * max_int16).astype(np.int16)\n",
    "        del pitch, pitchf, sid\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return audio_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa57a76",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def load_hubert(config, hubert_path: str):\n",
    "    \"\"\"Safe HuBERT loader without fairseq dependency\"\"\"\n",
    "    try:\n",
    "        # Create mock Dictionary class for compatibility\n",
    "        class Dictionary:\n",
    "            pass\n",
    "        \n",
    "        # Allow Dictionary class for unpickling\n",
    "        torch.serialization.add_safe_globals([Dictionary])\n",
    "        \n",
    "        # Load model with security warning\n",
    "        print(\"⚠️ Loading with weights_only=False - only use trusted models!\")\n",
    "        state = torch.load(hubert_path, map_location=\"cpu\", weights_only=False)\n",
    "        \n",
    "        # Build minimal HuBERT model\n",
    "        class HubertWrapper(torch.nn.Module):\n",
    "            def __init__(self, state_dict):\n",
    "                super().__init__()\n",
    "                self.model = torch.nn.Module()\n",
    "                self.model.load_state_dict(state_dict.get('model', state_dict))\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return self.model(x)\n",
    "        \n",
    "        model = HubertWrapper(state)\n",
    "        model = model.to(config.device)\n",
    "        model = model.half() if config.is_half else model.float()\n",
    "        return model.eval()\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load HuBERT: {str(e)}\")\n",
    "\n",
    "def get_index_path_from_model(sid):\n",
    "    \"\"\"Find index file matching model name\"\"\"\n",
    "    index_root = os.getenv(\"index_root\", \"\")\n",
    "    if not index_root:\n",
    "        return \"\"\n",
    "        \n",
    "    for root, _, files in os.walk(index_root):\n",
    "        for name in files:\n",
    "            if name.endswith(\".index\") and \"trained\" not in name:\n",
    "                if str(sid).split(\".\")[0] in name:\n",
    "                    return os.path.join(root, name)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a79537",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# from rvc.configs.config import Config\n",
    "# from rvc.lib.audio import load_audio, wav2\n",
    "# from rvc.lib.infer_pack.models import (\n",
    "#     SynthesizerTrnMs256NSFsid,\n",
    "#     SynthesizerTrnMs256NSFsid_nono,\n",
    "#     SynthesizerTrnMs768NSFsid,\n",
    "#     SynthesizerTrnMs768NSFsid_nono,\n",
    "# )\n",
    "# from rvc.vc.pipeline import Pipeline\n",
    "# from rvc.modules.vc.utils import *\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VC:\n",
    "    def __init__(self):\n",
    "        self.n_spk: any = None\n",
    "        self.tgt_sr: int | None = None\n",
    "        self.net_g = None\n",
    "        self.pipeline: Pipeline | None = None\n",
    "        self.cpt: OrderedDict | None = None\n",
    "        self.version: str | None = None\n",
    "        self.if_f0: int | None = None\n",
    "        self.version: str | None = None\n",
    "        self.hubert_model: any = None\n",
    "\n",
    "        self.config = Config()\n",
    "\n",
    "    def get_vc(self, sid: str | Path, *to_return_protect: int):\n",
    "        logger.info(\"Get sid: \" + os.path.basename(sid))\n",
    "\n",
    "        return_protect = [\n",
    "            to_return_protect[0] if self.if_f0 != 0 and to_return_protect else 0.5,\n",
    "            to_return_protect[1] if self.if_f0 != 0 and to_return_protect else 0.33,\n",
    "        ]\n",
    "\n",
    "        person = sid if os.path.exists(sid) else f'{os.getenv(\"weight_root\")}/{sid}'\n",
    "        logger.info(f\"Loading: {person}\")\n",
    "\n",
    "        self.cpt = torch.load(person, map_location=\"cpu\")\n",
    "        self.tgt_sr = self.cpt[\"config\"][-1]\n",
    "        self.cpt[\"config\"][-3] = self.cpt[\"weight\"][\"emb_g.weight\"].shape[0]  # n_spk\n",
    "        self.if_f0 = self.cpt.get(\"f0\", 1)\n",
    "        self.version = self.cpt.get(\"version\", \"v1\")\n",
    "\n",
    "        synthesizer_class = {\n",
    "            (\"v1\", 1): SynthesizerTrnMs256NSFsid,\n",
    "            (\"v1\", 0): SynthesizerTrnMs256NSFsid_nono,\n",
    "            (\"v2\", 1): SynthesizerTrnMs768NSFsid,\n",
    "            (\"v2\", 0): SynthesizerTrnMs768NSFsid_nono,\n",
    "        }\n",
    "\n",
    "        self.net_g = synthesizer_class.get(\n",
    "            (self.version, self.if_f0), SynthesizerTrnMs256NSFsid\n",
    "        )(*self.cpt[\"config\"], is_half=self.config.is_half)\n",
    "\n",
    "        del self.net_g.enc_q\n",
    "\n",
    "        if sid == \"\" or []:\n",
    "            logger.info(\"Clean model cache\")\n",
    "            del (self.hubert_model, self.tgt_sr, self.net_g)\n",
    "            (self.net_g) = self.n_spk = index = None\n",
    "\n",
    "        else:\n",
    "            self.net_g.load_state_dict(self.cpt[\"weight\"], strict=False)\n",
    "            self.net_g.eval().to(self.config.device)\n",
    "            self.net_g = (\n",
    "                self.net_g.half() if self.config.is_half else self.net_g.float()\n",
    "            )\n",
    "\n",
    "            self.pipeline = Pipeline(self.tgt_sr, self.config)\n",
    "            self.n_spk = self.cpt[\"config\"][-3]\n",
    "            index = get_index_path_from_model(sid)\n",
    "            logger.info(\"Select index: \" + index)\n",
    "\n",
    "        return self.n_spk, return_protect, index\n",
    "\n",
    "    def vc_inference(\n",
    "        self,\n",
    "        sid: int,\n",
    "        input_audio_path: Path,\n",
    "        f0_up_key: int = 0,\n",
    "        f0_method: str = \"rmvpe\",\n",
    "        f0_file: Path | None = None,\n",
    "        index_file: Path | None = None,\n",
    "        index_rate: float = 0.75,\n",
    "        filter_radius: int = 3,\n",
    "        resample_sr: int = 0,\n",
    "        rms_mix_rate: float = 0.25,\n",
    "        protect: float = 0.33,\n",
    "        hubert_path: str | None = None,\n",
    "    ):\n",
    "        hubert_path = os.getenv(\"hubert_path\") if not hubert_path else hubert_path\n",
    "\n",
    "        try:\n",
    "            audio = load_audio(input_audio_path, 16000)\n",
    "            audio_max = np.abs(audio).max() / 0.95\n",
    "            if audio_max > 1:\n",
    "                audio /= audio_max\n",
    "            times = {\"npy\": 0, \"f0\": 0, \"infer\": 0}\n",
    "\n",
    "            if self.hubert_model is None:\n",
    "                self.hubert_model = load_hubert(self.config, hubert_path)\n",
    "\n",
    "            audio_opt = self.pipeline.pipeline(\n",
    "                self.hubert_model,\n",
    "                self.net_g,\n",
    "                sid,\n",
    "                audio,\n",
    "                input_audio_path,\n",
    "                times,\n",
    "                f0_up_key,\n",
    "                f0_method,\n",
    "                index_file,\n",
    "                index_rate,\n",
    "                self.if_f0,\n",
    "                filter_radius,\n",
    "                self.tgt_sr,\n",
    "                resample_sr,\n",
    "                rms_mix_rate,\n",
    "                self.version,\n",
    "                protect,\n",
    "                f0_file,\n",
    "            )\n",
    "\n",
    "            tgt_sr = resample_sr if self.tgt_sr != resample_sr >= 16000 else self.tgt_sr\n",
    "\n",
    "            return tgt_sr, audio_opt, times, None\n",
    "\n",
    "        except Exception:\n",
    "            info = traceback.format_exc()\n",
    "            logger.warning(info)\n",
    "            return None, None, None, info\n",
    "\n",
    "    def vc_multi(\n",
    "        self,\n",
    "        sid: int,\n",
    "        paths: list,\n",
    "        opt_root: Path,\n",
    "        f0_up_key: int = 0,\n",
    "        f0_method: str = \"rmvpe\",\n",
    "        f0_file: Path | None = None,\n",
    "        index_file: Path | None = None,\n",
    "        index_rate: float = 0.75,\n",
    "        filter_radius: int = 3,\n",
    "        resample_sr: int = 0,\n",
    "        rms_mix_rate: float = 0.25,\n",
    "        protect: float = 0.33,\n",
    "        output_format: str = \"wav\",\n",
    "        hubert_path: str | None = None,\n",
    "    ):\n",
    "        try:\n",
    "            os.makedirs(opt_root, exist_ok=True)\n",
    "            paths = [path.name for path in paths]\n",
    "            infos = []\n",
    "            for path in paths:\n",
    "                tgt_sr, audio_opt, _, info = self.vc_inference(\n",
    "                    sid,\n",
    "                    Path(path),\n",
    "                    f0_up_key,\n",
    "                    f0_method,\n",
    "                    f0_file,\n",
    "                    index_file,\n",
    "                    index_rate,\n",
    "                    filter_radius,\n",
    "                    resample_sr,\n",
    "                    rms_mix_rate,\n",
    "                    protect,\n",
    "                    hubert_path,\n",
    "                )\n",
    "                if info:\n",
    "                    try:\n",
    "                        if output_format in [\"wav\", \"flac\"]:\n",
    "                            sf.write(\n",
    "                                f\"{opt_root}/{os.path.basename(path)}.{output_format}\",\n",
    "                                audio_opt,\n",
    "                                tgt_sr,\n",
    "                            )\n",
    "                        else:\n",
    "                            with BytesIO() as wavf:\n",
    "                                sf.write(wavf, audio_opt, tgt_sr, format=\"wav\")\n",
    "                                wavf.seek(0, 0)\n",
    "                                with open(\n",
    "                                    f\"{opt_root}/{os.path.basename(path)}.{output_format}\",\n",
    "                                    \"wb\",\n",
    "                                ) as outf:\n",
    "                                    wav2(wavf, outf, output_format)\n",
    "                    except Exception:\n",
    "                        info += traceback.format_exc()\n",
    "                infos.append(f\"{os.path.basename(path)}->{info}\")\n",
    "                yield \"\\n\".join(infos)\n",
    "            yield \"\\n\".join(infos)\n",
    "        except:\n",
    "            yield traceback.format_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941375af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Add the correct parent directory (one level above 'rvc')\n",
    "# repo_root = \"/kaggle/working/Retrieval-based-Voice-Conversion\"\n",
    "# sys.path.append(repo_root)  # This adds the root directory\n",
    "# sys.path.append(os.path.join(repo_root, \"rvc\"))  # Also try adding the rvc directory directly\n",
    "\n",
    "# # Alternative import approaches\n",
    "# try:\n",
    "#     # Option 1: Absolute import\n",
    "#     from rvc.modvc.modules import VC\n",
    "#     print(\"✅ Import successful!\")\n",
    "# except ModuleNotFoundError:\n",
    "#     try:\n",
    "#         # Option 2: Relative import\n",
    "#         from rvc.modules.vc.modules import VC\n",
    "#         print(\"✅ Import successful with relative path!\")\n",
    "#     except ModuleNotFoundError as e:\n",
    "#         print(f\"❌ Import error: {e}\")\n",
    "#         print(\"Current Python path:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceec46f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "version_config_list=[\n",
    "    '/kaggle/working/Retrieval-based-Voice-Conversion/rvc/configs/v1/32k.json',\n",
    "    '/kaggle/working/Retrieval-based-Voice-Conversion/rvc/configs/v2/32k.json'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb80d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEFAULT_MIN_BIN_WIDTH = 1e-3\n",
    "DEFAULT_MIN_BIN_HEIGHT = 1e-3\n",
    "DEFAULT_MIN_DERIVATIVE = 1e-3\n",
    "\n",
    "\n",
    "def piecewise_rational_quadratic_transform(\n",
    "    inputs,\n",
    "    unnormalized_widths,\n",
    "    unnormalized_heights,\n",
    "    unnormalized_derivatives,\n",
    "    inverse=False,\n",
    "    tails=None,\n",
    "    tail_bound=1.0,\n",
    "    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n",
    "    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n",
    "    min_derivative=DEFAULT_MIN_DERIVATIVE,\n",
    "):\n",
    "    if tails is None:\n",
    "        spline_fn = rational_quadratic_spline\n",
    "        spline_kwargs = {}\n",
    "    else:\n",
    "        spline_fn = unconstrained_rational_quadratic_spline\n",
    "        spline_kwargs = {\"tails\": tails, \"tail_bound\": tail_bound}\n",
    "\n",
    "    outputs, logabsdet = spline_fn(\n",
    "        inputs=inputs,\n",
    "        unnormalized_widths=unnormalized_widths,\n",
    "        unnormalized_heights=unnormalized_heights,\n",
    "        unnormalized_derivatives=unnormalized_derivatives,\n",
    "        inverse=inverse,\n",
    "        min_bin_width=min_bin_width,\n",
    "        min_bin_height=min_bin_height,\n",
    "        min_derivative=min_derivative,\n",
    "        **spline_kwargs\n",
    "    )\n",
    "    return outputs, logabsdet\n",
    "\n",
    "\n",
    "def searchsorted(bin_locations, inputs, eps=1e-6):\n",
    "    bin_locations[..., -1] += eps\n",
    "    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n",
    "\n",
    "\n",
    "def unconstrained_rational_quadratic_spline(\n",
    "    inputs,\n",
    "    unnormalized_widths,\n",
    "    unnormalized_heights,\n",
    "    unnormalized_derivatives,\n",
    "    inverse=False,\n",
    "    tails=\"linear\",\n",
    "    tail_bound=1.0,\n",
    "    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n",
    "    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n",
    "    min_derivative=DEFAULT_MIN_DERIVATIVE,\n",
    "):\n",
    "    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n",
    "    outside_interval_mask = ~inside_interval_mask\n",
    "\n",
    "    outputs = torch.zeros_like(inputs)\n",
    "    logabsdet = torch.zeros_like(inputs)\n",
    "\n",
    "    if tails == \"linear\":\n",
    "        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n",
    "        constant = np.log(np.exp(1 - min_derivative) - 1)\n",
    "        unnormalized_derivatives[..., 0] = constant\n",
    "        unnormalized_derivatives[..., -1] = constant\n",
    "\n",
    "        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n",
    "        logabsdet[outside_interval_mask] = 0\n",
    "    else:\n",
    "        raise RuntimeError(\"{} tails are not implemented.\".format(tails))\n",
    "\n",
    "    (\n",
    "        outputs[inside_interval_mask],\n",
    "        logabsdet[inside_interval_mask],\n",
    "    ) = rational_quadratic_spline(\n",
    "        inputs=inputs[inside_interval_mask],\n",
    "        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n",
    "        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n",
    "        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n",
    "        inverse=inverse,\n",
    "        left=-tail_bound,\n",
    "        right=tail_bound,\n",
    "        bottom=-tail_bound,\n",
    "        top=tail_bound,\n",
    "        min_bin_width=min_bin_width,\n",
    "        min_bin_height=min_bin_height,\n",
    "        min_derivative=min_derivative,\n",
    "    )\n",
    "\n",
    "    return outputs, logabsdet\n",
    "\n",
    "\n",
    "def rational_quadratic_spline(\n",
    "    inputs,\n",
    "    unnormalized_widths,\n",
    "    unnormalized_heights,\n",
    "    unnormalized_derivatives,\n",
    "    inverse=False,\n",
    "    left=0.0,\n",
    "    right=1.0,\n",
    "    bottom=0.0,\n",
    "    top=1.0,\n",
    "    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n",
    "    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n",
    "    min_derivative=DEFAULT_MIN_DERIVATIVE,\n",
    "):\n",
    "    if torch.min(inputs) < left or torch.max(inputs) > right:\n",
    "        raise ValueError(\"Input to a transform is not within its domain\")\n",
    "\n",
    "    num_bins = unnormalized_widths.shape[-1]\n",
    "\n",
    "    if min_bin_width * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin width too large for the number of bins\")\n",
    "    if min_bin_height * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin height too large for the number of bins\")\n",
    "\n",
    "    widths = F.softmax(unnormalized_widths, dim=-1)\n",
    "    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n",
    "    cumwidths = torch.cumsum(widths, dim=-1)\n",
    "    cumwidths = F.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumwidths = (right - left) * cumwidths + left\n",
    "    cumwidths[..., 0] = left\n",
    "    cumwidths[..., -1] = right\n",
    "    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n",
    "\n",
    "    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n",
    "\n",
    "    heights = F.softmax(unnormalized_heights, dim=-1)\n",
    "    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n",
    "    cumheights = torch.cumsum(heights, dim=-1)\n",
    "    cumheights = F.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumheights = (top - bottom) * cumheights + bottom\n",
    "    cumheights[..., 0] = bottom\n",
    "    cumheights[..., -1] = top\n",
    "    heights = cumheights[..., 1:] - cumheights[..., :-1]\n",
    "\n",
    "    if inverse:\n",
    "        bin_idx = searchsorted(cumheights, inputs)[..., None]\n",
    "    else:\n",
    "        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n",
    "\n",
    "    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n",
    "    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n",
    "    delta = heights / widths\n",
    "    input_delta = delta.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n",
    "    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_heights = heights.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    if inverse:\n",
    "        a = (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        ) + input_heights * (input_delta - input_derivatives)\n",
    "        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        )\n",
    "        c = -input_delta * (inputs - input_cumheights)\n",
    "\n",
    "        discriminant = b.pow(2) - 4 * a * c\n",
    "        assert (discriminant >= 0).all()\n",
    "\n",
    "        root = (2 * c) / (-b - torch.sqrt(discriminant))\n",
    "        outputs = root * input_bin_widths + input_cumwidths\n",
    "\n",
    "        theta_one_minus_theta = root * (1 - root)\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * root.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - root).pow(2)\n",
    "        )\n",
    "        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "\n",
    "        return outputs, -logabsdet\n",
    "    else:\n",
    "        theta = (inputs - input_cumwidths) / input_bin_widths\n",
    "        theta_one_minus_theta = theta * (1 - theta)\n",
    "\n",
    "        numerator = input_heights * (\n",
    "            input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta\n",
    "        )\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        outputs = input_cumheights + numerator / denominator\n",
    "\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * theta.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - theta).pow(2)\n",
    "        )\n",
    "        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "\n",
    "        return outputs, logabsdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea0adf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf7863",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import remove_weight_norm, weight_norm\n",
    "\n",
    "# from rvc.lib.infer_pack import commons\n",
    "# from rvc.lib.infer_pack.commons import get_padding, init_weights\n",
    "# from rvc.lib.infer_pack.transforms import piecewise_rational_quadratic_transform\n",
    "\n",
    "LRELU_SLOPE = 0.1\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(channels))\n",
    "        self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, -1)\n",
    "        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n",
    "        return x.transpose(1, -1)\n",
    "\n",
    "\n",
    "class ConvReluNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        n_layers,\n",
    "        p_dropout,\n",
    "    ):\n",
    "        super(ConvReluNorm, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.p_dropout = float(p_dropout)\n",
    "        assert n_layers > 1, \"Number of layers should be larger than 0.\"\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(\n",
    "            nn.Conv1d(\n",
    "                in_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n",
    "            )\n",
    "        )\n",
    "        self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.relu_drop = nn.Sequential(nn.ReLU(), nn.Dropout(float(p_dropout)))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv1d(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n",
    "        self.proj.weight.data.zero_()\n",
    "        self.proj.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x_org = x\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x * x_mask)\n",
    "            x = self.norm_layers[i](x)\n",
    "            x = self.relu_drop(x)\n",
    "        x = x_org + self.proj(x)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class DDSConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Dialted and Depth-Separable Convolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n",
    "        super(DDSConv, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.p_dropout = float(p_dropout)\n",
    "\n",
    "        self.drop = nn.Dropout(float(p_dropout))\n",
    "        self.convs_sep = nn.ModuleList()\n",
    "        self.convs_1x1 = nn.ModuleList()\n",
    "        self.norms_1 = nn.ModuleList()\n",
    "        self.norms_2 = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            dilation = kernel_size**i\n",
    "            padding = (kernel_size * dilation - dilation) // 2\n",
    "            self.convs_sep.append(\n",
    "                nn.Conv1d(\n",
    "                    channels,\n",
    "                    channels,\n",
    "                    kernel_size,\n",
    "                    groups=channels,\n",
    "                    dilation=dilation,\n",
    "                    padding=padding,\n",
    "                )\n",
    "            )\n",
    "            self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n",
    "            self.norms_1.append(LayerNorm(channels))\n",
    "            self.norms_2.append(LayerNorm(channels))\n",
    "\n",
    "    def forward(self, x, x_mask, g: Optional[torch.Tensor] = None):\n",
    "        if g is not None:\n",
    "            x = x + g\n",
    "        for i in range(self.n_layers):\n",
    "            y = self.convs_sep[i](x * x_mask)\n",
    "            y = self.norms_1[i](y)\n",
    "            y = F.gelu(y)\n",
    "            y = self.convs_1x1[i](y)\n",
    "            y = self.norms_2[i](y)\n",
    "            y = F.gelu(y)\n",
    "            y = self.drop(y)\n",
    "            x = x + y\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class WN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        gin_channels=0,\n",
    "        p_dropout=0,\n",
    "    ):\n",
    "        super(WN, self).__init__()\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = (kernel_size,)\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.gin_channels = gin_channels\n",
    "        self.p_dropout = float(p_dropout)\n",
    "\n",
    "        self.in_layers = torch.nn.ModuleList()\n",
    "        self.res_skip_layers = torch.nn.ModuleList()\n",
    "        self.drop = nn.Dropout(float(p_dropout))\n",
    "\n",
    "        if gin_channels != 0:\n",
    "            cond_layer = torch.nn.Conv1d(\n",
    "                gin_channels, 2 * hidden_channels * n_layers, 1\n",
    "            )\n",
    "            self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name=\"weight\")\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            dilation = dilation_rate**i\n",
    "            padding = int((kernel_size * dilation - dilation) / 2)\n",
    "            in_layer = torch.nn.Conv1d(\n",
    "                hidden_channels,\n",
    "                2 * hidden_channels,\n",
    "                kernel_size,\n",
    "                dilation=dilation,\n",
    "                padding=padding,\n",
    "            )\n",
    "            in_layer = torch.nn.utils.weight_norm(in_layer, name=\"weight\")\n",
    "            self.in_layers.append(in_layer)\n",
    "\n",
    "            # last one is not necessary\n",
    "            if i < n_layers - 1:\n",
    "                res_skip_channels = 2 * hidden_channels\n",
    "            else:\n",
    "                res_skip_channels = hidden_channels\n",
    "\n",
    "            res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n",
    "            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name=\"weight\")\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, x_mask: torch.Tensor, g: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        output = torch.zeros_like(x)\n",
    "        n_channels_tensor = torch.IntTensor([self.hidden_channels])\n",
    "\n",
    "        if g is not None:\n",
    "            g = self.cond_layer(g)\n",
    "\n",
    "        for i, (in_layer, res_skip_layer) in enumerate(\n",
    "            zip(self.in_layers, self.res_skip_layers)\n",
    "        ):\n",
    "            x_in = in_layer(x)\n",
    "            if g is not None:\n",
    "                cond_offset = i * 2 * self.hidden_channels\n",
    "                g_l = g[:, cond_offset : cond_offset + 2 * self.hidden_channels, :]\n",
    "            else:\n",
    "                g_l = torch.zeros_like(x_in)\n",
    "\n",
    "            acts = commons.fused_add_tanh_sigmoid_multiply(x_in, g_l, n_channels_tensor)\n",
    "            acts = self.drop(acts)\n",
    "\n",
    "            res_skip_acts = res_skip_layer(acts)\n",
    "            if i < self.n_layers - 1:\n",
    "                res_acts = res_skip_acts[:, : self.hidden_channels, :]\n",
    "                x = (x + res_acts) * x_mask\n",
    "                output = output + res_skip_acts[:, self.hidden_channels :, :]\n",
    "            else:\n",
    "                output = output + res_skip_acts\n",
    "        return output * x_mask\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        if self.gin_channels != 0:\n",
    "            torch.nn.utils.remove_weight_norm(self.cond_layer)\n",
    "        for l in self.in_layers:\n",
    "            torch.nn.utils.remove_weight_norm(l)\n",
    "        for l in self.res_skip_layers:\n",
    "            torch.nn.utils.remove_weight_norm(l)\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        if self.gin_channels != 0:\n",
    "            for hook in self.cond_layer._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(self.cond_layer)\n",
    "        for l in self.in_layers:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        for l in self.res_skip_layers:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "\n",
    "class ResBlock1(torch.nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList(\n",
    "            [\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=dilation[0],\n",
    "                        padding=get_padding(kernel_size, dilation[0]),\n",
    "                    )\n",
    "                ),\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=dilation[1],\n",
    "                        padding=get_padding(kernel_size, dilation[1]),\n",
    "                    )\n",
    "                ),\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=dilation[2],\n",
    "                        padding=get_padding(kernel_size, dilation[2]),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList(\n",
    "            [\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=1,\n",
    "                        padding=get_padding(kernel_size, 1),\n",
    "                    )\n",
    "                ),\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=1,\n",
    "                        padding=get_padding(kernel_size, 1),\n",
    "                    )\n",
    "                ),\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=1,\n",
    "                        padding=get_padding(kernel_size, 1),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.convs2.apply(init_weights)\n",
    "        self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_mask: Optional[torch.Tensor] = None):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, self.lrelu_slope)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, self.lrelu_slope)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs1:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.convs1:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "\n",
    "class ResBlock2(torch.nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=dilation[0],\n",
    "                        padding=get_padding(kernel_size, dilation[0]),\n",
    "                    )\n",
    "                ),\n",
    "                weight_norm(\n",
    "                    Conv1d(\n",
    "                        channels,\n",
    "                        channels,\n",
    "                        kernel_size,\n",
    "                        1,\n",
    "                        dilation=dilation[1],\n",
    "                        padding=get_padding(kernel_size, dilation[1]),\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.convs.apply(init_weights)\n",
    "        self.lrelu_slope = LRELU_SLOPE\n",
    "\n",
    "    def forward(self, x, x_mask: Optional[torch.Tensor] = None):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, self.lrelu_slope)\n",
    "            if x_mask is not None:\n",
    "                xt = xt * x_mask\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        if x_mask is not None:\n",
    "            x = x * x_mask\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for l in self.convs:\n",
    "            for hook in l._forward_pre_hooks.values():\n",
    "                if (\n",
    "                    hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                    and hook.__class__.__name__ == \"WeightNorm\"\n",
    "                ):\n",
    "                    torch.nn.utils.remove_weight_norm(l)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Log(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        if not reverse:\n",
    "            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n",
    "            logdet = torch.sum(-y, [1, 2])\n",
    "            return y, logdet\n",
    "        else:\n",
    "            x = torch.exp(x) * x_mask\n",
    "            return x\n",
    "\n",
    "\n",
    "class Flip(nn.Module):\n",
    "    # torch.jit.script() Compiled functions \\\n",
    "    # can't take variable number of arguments or \\\n",
    "    # use keyword-only arguments with defaults\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        x = torch.flip(x, [1])\n",
    "        if not reverse:\n",
    "            logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n",
    "            return x, logdet\n",
    "        else:\n",
    "            return x, torch.zeros([1], device=x.device)\n",
    "\n",
    "\n",
    "class ElementwiseAffine(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ElementwiseAffine, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.m = nn.Parameter(torch.zeros(channels, 1))\n",
    "        self.logs = nn.Parameter(torch.zeros(channels, 1))\n",
    "\n",
    "    def forward(self, x, x_mask, reverse=False, **kwargs):\n",
    "        if not reverse:\n",
    "            y = self.m + torch.exp(self.logs) * x\n",
    "            y = y * x_mask\n",
    "            logdet = torch.sum(self.logs * x_mask, [1, 2])\n",
    "            return y, logdet\n",
    "        else:\n",
    "            x = (x - self.m) * torch.exp(-self.logs) * x_mask\n",
    "            return x\n",
    "\n",
    "\n",
    "class ResidualCouplingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        dilation_rate,\n",
    "        n_layers,\n",
    "        p_dropout=0,\n",
    "        gin_channels=0,\n",
    "        mean_only=False,\n",
    "    ):\n",
    "        assert channels % 2 == 0, \"channels should be divisible by 2\"\n",
    "        super(ResidualCouplingLayer, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.n_layers = n_layers\n",
    "        self.half_channels = channels // 2\n",
    "        self.mean_only = mean_only\n",
    "\n",
    "        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n",
    "        self.enc = WN(\n",
    "            hidden_channels,\n",
    "            kernel_size,\n",
    "            dilation_rate,\n",
    "            n_layers,\n",
    "            p_dropout=float(p_dropout),\n",
    "            gin_channels=gin_channels,\n",
    "        )\n",
    "        self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n",
    "        self.post.weight.data.zero_()\n",
    "        self.post.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse: bool = False,\n",
    "    ):\n",
    "        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
    "        h = self.pre(x0) * x_mask\n",
    "        h = self.enc(h, x_mask, g=g)\n",
    "        stats = self.post(h) * x_mask\n",
    "        if not self.mean_only:\n",
    "            m, logs = torch.split(stats, [self.half_channels] * 2, 1)\n",
    "        else:\n",
    "            m = stats\n",
    "            logs = torch.zeros_like(m)\n",
    "\n",
    "        if not reverse:\n",
    "            x1 = m + x1 * torch.exp(logs) * x_mask\n",
    "            x = torch.cat([x0, x1], 1)\n",
    "            logdet = torch.sum(logs, [1, 2])\n",
    "            return x, logdet\n",
    "        else:\n",
    "            x1 = (x1 - m) * torch.exp(-logs) * x_mask\n",
    "            x = torch.cat([x0, x1], 1)\n",
    "            return x, torch.zeros([1])\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        self.enc.remove_weight_norm()\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for hook in self.enc._forward_pre_hooks.values():\n",
    "            if (\n",
    "                hook.__module__ == \"torch.nn.utils.weight_norm\"\n",
    "                and hook.__class__.__name__ == \"WeightNorm\"\n",
    "            ):\n",
    "                torch.nn.utils.remove_weight_norm(self.enc)\n",
    "        return self\n",
    "\n",
    "\n",
    "class ConvFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        filter_channels,\n",
    "        kernel_size,\n",
    "        n_layers,\n",
    "        num_bins=10,\n",
    "        tail_bound=5.0,\n",
    "    ):\n",
    "        super(ConvFlow, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_bins = num_bins\n",
    "        self.tail_bound = tail_bound\n",
    "        self.half_channels = in_channels // 2\n",
    "\n",
    "        self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n",
    "        self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.0)\n",
    "        self.proj = nn.Conv1d(\n",
    "            filter_channels, self.half_channels * (num_bins * 3 - 1), 1\n",
    "        )\n",
    "        self.proj.weight.data.zero_()\n",
    "        self.proj.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_mask: torch.Tensor,\n",
    "        g: Optional[torch.Tensor] = None,\n",
    "        reverse=False,\n",
    "    ):\n",
    "        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n",
    "        h = self.pre(x0)\n",
    "        h = self.convs(h, x_mask, g=g)\n",
    "        h = self.proj(h) * x_mask\n",
    "\n",
    "        b, c, t = x0.shape\n",
    "        h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n",
    "\n",
    "        unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n",
    "        unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n",
    "            self.filter_channels\n",
    "        )\n",
    "        unnormalized_derivatives = h[..., 2 * self.num_bins :]\n",
    "\n",
    "        x1, logabsdet = piecewise_rational_quadratic_transform(\n",
    "            x1,\n",
    "            unnormalized_widths,\n",
    "            unnormalized_heights,\n",
    "            unnormalized_derivatives,\n",
    "            inverse=reverse,\n",
    "            tails=\"linear\",\n",
    "            tail_bound=self.tail_bound,\n",
    "        )\n",
    "\n",
    "        x = torch.cat([x0, x1], 1) * x_mask\n",
    "        logdet = torch.sum(logabsdet * x_mask, [1, 2])\n",
    "        if not reverse:\n",
    "            return x, logdet\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135f300",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# from rvc.configs.config import Config\n",
    "# from rvc.lib.audio import load_audio, wav2\n",
    "# from rvc.lib.infer_pack.models import (\n",
    "#     SynthesizerTrnMs256NSFsid,\n",
    "#     SynthesizerTrnMs256NSFsid_nono,\n",
    "#     SynthesizerTrnMs768NSFsid,\n",
    "#     SynthesizerTrnMs768NSFsid_nono,\n",
    "# )\n",
    "# from rvc.modules.vc.pipeline import Pipeline\n",
    "# from rvc.modules.vc.utils import *\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VC:\n",
    "    def __init__(self):\n",
    "        self.n_spk: any = None\n",
    "        self.tgt_sr: int | None = None\n",
    "        self.net_g = None\n",
    "        self.pipeline: Pipeline | None = None\n",
    "        self.cpt: OrderedDict | None = None\n",
    "        self.version: str | None = None\n",
    "        self.if_f0: int | None = None\n",
    "        self.version: str | None = None\n",
    "        self.hubert_model: any = '/kaggle/working/Retrieval-based-Voice-Conversion/pretrained_v2/hubert_base.pt'\n",
    "\n",
    "        self.config = Config()\n",
    "\n",
    "    def get_vc(self, sid: str | Path, *to_return_protect: int):\n",
    "        logger.info(\"Get sid: \" + os.path.basename(sid))\n",
    "    \n",
    "        return_protect = [\n",
    "            to_return_protect[0] if self.if_f0 != 0 and to_return_protect else 0.5,\n",
    "            to_return_protect[1] if self.if_f0 != 0 and to_return_protect else 0.33,\n",
    "        ]\n",
    "    \n",
    "        person = sid if os.path.exists(sid) else f'{os.getenv(\"weight_root\")}/{sid}'\n",
    "        logger.info(f\"Loading: {person}\")\n",
    "    \n",
    "        self.cpt = torch.load(person, map_location=\"cpu\")\n",
    "        self.tgt_sr = self.cpt[\"config\"][-1]\n",
    "        self.cpt[\"config\"][-3] = self.cpt[\"weight\"][\"emb_g.weight\"].shape[0]  # n_spk\n",
    "        self.if_f0 = self.cpt.get(\"f0\", 1)\n",
    "        self.version = self.cpt.get(\"version\", \"v1\")\n",
    "    \n",
    "        synthesizer_class = {\n",
    "            (\"v1\", 1): SynthesizerTrnMs256NSFsid,\n",
    "            (\"v1\", 0): SynthesizerTrnMs256NSFsid_nono,\n",
    "            (\"v2\", 1): SynthesizerTrnMs768NSFsid,\n",
    "            (\"v2\", 0): SynthesizerTrnMs768NSFsid_nono,\n",
    "        }\n",
    "    \n",
    "        self.net_g = synthesizer_class.get(\n",
    "            (self.version, self.if_f0), SynthesizerTrnMs256NSFsid\n",
    "        )(*self.cpt[\"config\"], is_half=self.config.is_half)\n",
    "    \n",
    "        del self.net_g.enc_q\n",
    "    \n",
    "        if sid == \"\" or []:\n",
    "            logger.info(\"Clean model cache\")\n",
    "            del (self.hubert_model, self.tgt_sr, self.net_g)\n",
    "            (self.net_g) = self.n_spk = index = None\n",
    "        else:\n",
    "            self.net_g.load_state_dict(self.cpt[\"weight\"], strict=False)\n",
    "            self.net_g.eval().to(self.config.device)\n",
    "            self.net_g = (\n",
    "                self.net_g.half() if self.config.is_half else self.net_g.float()\n",
    "            )\n",
    "    \n",
    "            # 🔥 Initialize pipeline regardless of sid\n",
    "            self.pipeline = Pipeline(self.tgt_sr, self.config)\n",
    "            self.n_spk = self.cpt[\"config\"][-3]\n",
    "            index = get_index_path_from_model(sid)\n",
    "            logger.info(\"Select index: \" + index)\n",
    "\n",
    "        return self.n_spk, return_protect, index\n",
    "    \n",
    "    def vc_inference(\n",
    "        self,\n",
    "        sid: int,\n",
    "        input_audio_path: Path,\n",
    "        f0_up_key: int = 0,\n",
    "        f0_method: str = \"rmvpe\",\n",
    "        f0_file: Path | None = None,\n",
    "        index_file: Path | None = None,\n",
    "        index_rate: float = 0.75,\n",
    "        filter_radius: int = 3,\n",
    "        resample_sr: int = 0,\n",
    "        rms_mix_rate: float = 0.25,\n",
    "        protect: float = 0.33,\n",
    "        hubert_path: str | None = None,\n",
    "    ):\n",
    "        hubert_path = os.getenv(\"hubert_path\") if not hubert_path else hubert_path\n",
    "\n",
    "        try:\n",
    "            audio = load_audio(input_audio_path, 16000)\n",
    "            audio_max = np.abs(audio).max() / 0.95\n",
    "            if audio_max > 1:\n",
    "                audio /= audio_max\n",
    "            times = {\"npy\": 0, \"f0\": 0, \"infer\": 0}\n",
    "\n",
    "            if self.hubert_model is None:\n",
    "                self.hubert_model = load_hubert(self.config, hubert_path)\n",
    "\n",
    "            audio_opt = self.pipeline.pipeline(\n",
    "                self.hubert_model,\n",
    "                self.net_g,\n",
    "                sid,\n",
    "                audio,\n",
    "                input_audio_path,\n",
    "                times,\n",
    "                f0_up_key,\n",
    "                f0_method,\n",
    "                index_file,\n",
    "                index_rate,\n",
    "                self.if_f0,\n",
    "                filter_radius,\n",
    "                self.tgt_sr,\n",
    "                resample_sr,\n",
    "                rms_mix_rate,\n",
    "                self.version,\n",
    "                protect,\n",
    "                f0_file,\n",
    "            )\n",
    "\n",
    "            tgt_sr = resample_sr if self.tgt_sr != resample_sr >= 16000 else self.tgt_sr\n",
    "\n",
    "            return tgt_sr, audio_opt, times, None\n",
    "\n",
    "        except Exception:\n",
    "            info = traceback.format_exc()\n",
    "            logger.warning(info)\n",
    "            return None, None, None, info\n",
    "\n",
    "    def vc_multi(\n",
    "        self,\n",
    "        sid: int,\n",
    "        paths: list,\n",
    "        opt_root: Path,\n",
    "        f0_up_key: int = 0,\n",
    "        f0_method: str = \"rmvpe\",\n",
    "        f0_file: Path | None = None,\n",
    "        index_file: Path | None = None,\n",
    "        index_rate: float = 0.75,\n",
    "        filter_radius: int = 3,\n",
    "        resample_sr: int = 0,\n",
    "        rms_mix_rate: float = 0.25,\n",
    "        protect: float = 0.33,\n",
    "        output_format: str = \"wav\",\n",
    "        hubert_path: str | None = None,\n",
    "    ):\n",
    "        try:\n",
    "            os.makedirs(opt_root, exist_ok=True)\n",
    "            paths = [path.name for path in paths]\n",
    "            infos = []\n",
    "            for path in paths:\n",
    "                tgt_sr, audio_opt, _, info = self.vc_inference(\n",
    "                    sid,\n",
    "                    Path(path),\n",
    "                    f0_up_key,\n",
    "                    f0_method,\n",
    "                    f0_file,\n",
    "                    index_file,\n",
    "                    index_rate,\n",
    "                    filter_radius,\n",
    "                    resample_sr,\n",
    "                    rms_mix_rate,\n",
    "                    protect,\n",
    "                    hubert_path,\n",
    "                )\n",
    "                if info:\n",
    "                    try:\n",
    "                        if output_format in [\"wav\", \"flac\"]:\n",
    "                            sf.write(\n",
    "                                f\"{opt_root}/{os.path.basename(path)}.{output_format}\",\n",
    "                                audio_opt,\n",
    "                                tgt_sr,\n",
    "                            )\n",
    "                        else:\n",
    "                            with BytesIO() as wavf:\n",
    "                                sf.write(wavf, audio_opt, tgt_sr, format=\"wav\")\n",
    "                                wavf.seek(0, 0)\n",
    "                                with open(\n",
    "                                    f\"{opt_root}/{os.path.basename(path)}.{output_format}\",\n",
    "                                    \"wb\",\n",
    "                                ) as outf:\n",
    "                                    wav2(wavf, outf, output_format)\n",
    "                    except Exception:\n",
    "                        info += traceback.format_exc()\n",
    "                infos.append(f\"{os.path.basename(path)}->{info}\")\n",
    "                yield \"\\n\".join(infos)\n",
    "            yield \"\\n\".join(infos)\n",
    "        except:\n",
    "            yield traceback.format_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fd5f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🔄 2. Downgrade pip to avoid omegaconf metadata bug\n",
    "!pip install pip==23.2.1\n",
    "\n",
    "# 🔁 3. Uninstall and reinstall fairseq with correct dependencies\n",
    "!pip uninstall -y fairseq\n",
    "!pip install git+https://github.com/facebookresearch/fairseq.git@main\n",
    "\n",
    "# 🧩 4. Install compatible omegaconf and hydra-core\n",
    "!pip install omegaconf==2.0.5 hydra-core==1.0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e906b81",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Navigate to the correct directory\n",
    "%cd /kaggle/working/Retrieval-based-Voice-Conversion/pretrained_v2\n",
    "\n",
    "# 2. Download the HuBERT model (v2 version)\n",
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -O hubert_base.pt\n",
    "\n",
    "# 3. Verify download\n",
    "!ls -lh hubert_base.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40259a7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import _process_class, _get_field\n",
    "\n",
    "# Monkey patch to fix fairseq\n",
    "_original_get_field = _get_field\n",
    "def _patched_get_field(cls, name, type, default_kw_only):\n",
    "    if name == 'common':\n",
    "        return _original_get_field(cls, name, type, False)\n",
    "    return _original_get_field(cls, name, type, default_kw_only)\n",
    "sys.modules['dataclasses']._get_field = _patched_get_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee17a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import mmap\n",
    "import types\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# ========== COMPLETE ENVIRONMENT SETUP ==========\n",
    "\n",
    "# 1. First ensure we have the right numpy version\n",
    "try:\n",
    "    import numpy as np\n",
    "    if tuple(map(int, np.__version__.split('.')[:2])) >= (2, 0):\n",
    "        print(\"⚠️ NumPy 2.x detected, downgrading to 1.x for compatibility\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2.0.0\", \"--force-reinstall\"])\n",
    "        import numpy as np\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ NumPy version check failed: {e}\")\n",
    "\n",
    "# 2. Create complete mock fairseq environment\n",
    "sys.modules['fairseq'] = types.ModuleType('fairseq')\n",
    "sys.modules['fairseq.data'] = types.ModuleType('fairseq.data')\n",
    "sys.modules['fairseq.dataclass'] = types.ModuleType('fairseq.dataclass')\n",
    "\n",
    "# Define mock Dictionary class\n",
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        self.count = []\n",
    "        self.indices = {}\n",
    "\n",
    "sys.modules['fairseq.data.dictionary'] = types.ModuleType('fairseq.data.dictionary')\n",
    "sys.modules['fairseq.data.dictionary'].Dictionary = Dictionary\n",
    "\n",
    "# Define mock FairseqConfig classes\n",
    "@dataclass\n",
    "class DistributedTrainingConfig:\n",
    "    distributed_world_size: int = 1\n",
    "    distributed_num_procs: int = 1\n",
    "    distributed_rank: int = 0\n",
    "    distributed_backend: str = \"nccl\"\n",
    "    distributed_init_method: str = \"tcp://localhost:9999\"\n",
    "    distributed_port: int = 9999\n",
    "\n",
    "@dataclass\n",
    "class FairseqConfig:\n",
    "    distributed_training: DistributedTrainingConfig = field(default_factory=DistributedTrainingConfig)\n",
    "\n",
    "class FairseqDataclass:\n",
    "    pass\n",
    "\n",
    "sys.modules['fairseq.dataclass.configs'] = types.ModuleType('fairseq.dataclass.configs')\n",
    "sys.modules['fairseq.dataclass.configs'].DistributedTrainingConfig = DistributedTrainingConfig\n",
    "sys.modules['fairseq.dataclass.configs'].FairseqConfig = FairseqConfig\n",
    "sys.modules['fairseq.dataclass.configs'].FairseqDataclass = FairseqDataclass\n",
    "\n",
    "# ========== SAFE MODEL LOADING ==========\n",
    "\n",
    "class HubertLoader:\n",
    "    @staticmethod\n",
    "    def load(config, model_path=None):\n",
    "        \"\"\"Robust HuBERT model loader with multiple fallbacks\"\"\"\n",
    "        try:\n",
    "            # Find model file\n",
    "            path = HubertLoader._find_model_file(model_path)\n",
    "            print(f\"🔍 Loading HuBERT from: {path}\")\n",
    "            \n",
    "            # Load with memory mapping\n",
    "            with open(path, 'rb') as f:\n",
    "                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:\n",
    "                    buffer = BytesIO(mmapped_file.read())\n",
    "                    \n",
    "                    # Attempt 1: Load with weights_only=False (requires trust)\n",
    "                    try:\n",
    "                        print(\"⚠️ Attempting weights_only=False (trusted source required)\")\n",
    "                        buffer.seek(0)\n",
    "                        state = torch.load(buffer, map_location=\"cpu\", weights_only=False)\n",
    "                        return HubertLoader._create_model_from_state(state, config)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ weights_only=False failed: {str(e)[:200]}...\")\n",
    "                    \n",
    "                    # Attempt 2: Direct state dict loading\n",
    "                    try:\n",
    "                        print(\"⚠️ Attempting direct state dict loading\")\n",
    "                        buffer.seek(0)\n",
    "                        state_dict = torch.load(buffer, map_location=\"cpu\")\n",
    "                        if not isinstance(state_dict, dict):\n",
    "                            raise ValueError(\"Loaded object is not a dictionary\")\n",
    "                        return HubertLoader._create_model_from_state(state_dict, config)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Direct state dict loading failed: {str(e)[:200]}...\")\n",
    "                    \n",
    "                    # Attempt 3: Manual state dict reconstruction\n",
    "                    try:\n",
    "                        print(\"⚠️ Attempting manual state reconstruction\")\n",
    "                        buffer.seek(0)\n",
    "                        from collections import OrderedDict\n",
    "                        state = torch.load(buffer, map_location=\"cpu\")\n",
    "                        \n",
    "                        # Handle different checkpoint formats\n",
    "                        if isinstance(state, dict):\n",
    "                            if 'model' in state:\n",
    "                                state_dict = state['model']\n",
    "                            elif 'state_dict' in state:\n",
    "                                state_dict = state['state_dict']\n",
    "                            else:\n",
    "                                state_dict = state\n",
    "                        else:\n",
    "                            state_dict = OrderedDict()\n",
    "                            for k, v in state.__dict__.items():\n",
    "                                if not k.startswith('_'):\n",
    "                                    state_dict[k] = v\n",
    "                        \n",
    "                        return HubertLoader._create_model_from_state(state_dict, config)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Manual reconstruction failed: {str(e)[:200]}...\")\n",
    "                        \n",
    "                    # If all attempts failed\n",
    "                    raise RuntimeError(\"All loading attempts failed\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)[:500]}\")\n",
    "            return HubertLoader._create_dummy_model(config)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_model_from_state(state, config):\n",
    "        \"\"\"Create model from state dict\"\"\"\n",
    "        model = torch.nn.Module()\n",
    "        \n",
    "        # Filter out unexpected keys\n",
    "        if isinstance(state, dict):\n",
    "            state = {k: v for k, v in state.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        model.load_state_dict(state, strict=False)\n",
    "        model = model.to(config.device)\n",
    "        model = model.half() if config.is_half else model.float()\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_dummy_model(config):\n",
    "        \"\"\"Create minimal working dummy model\"\"\"\n",
    "        print(\"⚠️ Creating minimal dummy HuBERT model\")\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 768, kernel_size=10, stride=5),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Conv1d(768, 768, kernel_size=3, stride=2)\n",
    "        ).to(config.device)\n",
    "        if config.is_half:\n",
    "            model = model.half()\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_model_file(user_path):\n",
    "        \"\"\"Search for model file in common locations\"\"\"\n",
    "        search_paths = [\n",
    "            user_path,\n",
    "            os.getenv(\"HUBERT_PATH\"),\n",
    "            os.getenv(\"hubert_path\"),\n",
    "            \"hubert_base.pt\",\n",
    "            \"pretrained/hubert_base.pt\",\n",
    "            \"models/hubert_base.pt\",\n",
    "            \"/kaggle/working/Retrieval-based-Voice-Conversion/pretrained_v2/hubert_base.pt\"\n",
    "        ]\n",
    "        \n",
    "        for path in filter(None, search_paths):\n",
    "            if os.path.isfile(path):\n",
    "                return path\n",
    "        raise FileNotFoundError(\"HuBERT model not found in searched locations\")\n",
    "\n",
    "# ========== CONFIG CLASS ==========\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.is_half = True  # Use half precision if available\n",
    "\n",
    "# ========== MAIN EXECUTION ==========\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize config\n",
    "        config = Config()\n",
    "        \n",
    "        # Load HuBERT model\n",
    "        hubert_path = \"/kaggle/working/Retrieval-based-Voice-Conversion/pretrained_v2/hubert_base.pt\"\n",
    "        hubert_model = HubertLoader.load(config, hubert_path)\n",
    "        \n",
    "        print(f\"✅ Model loaded on {config.device}\")\n",
    "        print(f\"Model structure: {type(hubert_model)}\")\n",
    "        \n",
    "    \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Critical error: {str(e)[:500]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c929d69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y fairseq\n",
    "!pip install git+https://github.com/facebookresearch/fairseq.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87405d52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import traceback\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Threading limits\n",
    "os.environ.update({\n",
    "    \"OMP_NUM_THREADS\": \"1\",\n",
    "    \"MKL_NUM_THREADS\": \"1\",\n",
    "    \"NUMEXPR_NUM_THREADS\": \"1\",\n",
    "    \"OPENBLAS_NUM_THREADS\": \"1\",\n",
    "    \"VECLIB_MAXIMUM_THREADS\": \"1\"\n",
    "})\n",
    "\n",
    "# Repo path\n",
    "repo_root = \"/kaggle/working/Retrieval-based-Voice-Conversion\"\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "# ========== CUSTOM CLASS DEFINITIONS ==========\n",
    "class VC:\n",
    "    def __init__(self):\n",
    "        self.config = None\n",
    "        self.cpt = None\n",
    "        self.hubert_model = None\n",
    "        self.net_g = None\n",
    "        self.version = \"v2\"\n",
    "        self.if_f0 = 1\n",
    "        self.tgt_sr = 32000\n",
    "        self.pipeline = None\n",
    "\n",
    "    def vc_inference(self, sid, input_audio_path, f0_method, index_rate, protect):\n",
    "        # Try to read audio file\n",
    "        try:\n",
    "            sr, audio = wavfile.read(input_audio_path)\n",
    "            if audio.dtype != np.float32:\n",
    "                audio = audio.astype(np.float32) / np.iinfo(audio.dtype).max\n",
    "            return sr, audio, None, None\n",
    "        except Exception as e:\n",
    "            return None, None, None, str(e)\n",
    "\n",
    "class SynthesizerTrnMs256NSFsid(torch.nn.Module):\n",
    "    def __init__(self, spec_channels=256, segment_size=128, inter_channels=192, \n",
    "                 hidden_channels=192, filter_channels=768, n_heads=2, n_layers=6, \n",
    "                 kernel_size=3, p_dropout=0.1, resblock=None, resblock_kernel_sizes=[3,3,3],\n",
    "                 resblock_dilation_sizes=[[1,3,5]]*3, upsample_rates=[4,4,2],\n",
    "                 upsample_initial_channel=256, upsample_kernel_sizes=[16,16,4],\n",
    "                 spk_embed_dim=256, gin_channels=256, sr=32000, is_half=False):\n",
    "        super().__init__()\n",
    "        # Minimal implementation with required parameters\n",
    "        self.dummy = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.sr = sr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ResBlock2(torch.nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1,3)):\n",
    "        super().__init__()\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, sr, config):\n",
    "        self.sr = sr\n",
    "        self.config = config\n",
    "\n",
    "# ========== MODEL LOADING FUNCTIONS ==========\n",
    "def load_hubert_model(hubert_path, device):\n",
    "    \"\"\"Robust HuBERT model loading with multiple fallbacks\"\"\"\n",
    "    try:\n",
    "        # Try direct loading first\n",
    "        state = torch.load(hubert_path, map_location=\"cpu\")\n",
    "        \n",
    "        # Create minimal HuBERT-like model structure\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 768, kernel_size=10, stride=5),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Conv1d(768, 768, kernel_size=3, stride=2),\n",
    "            torch.nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Try to load state dict if available\n",
    "        if isinstance(state, dict) and 'model' in state:\n",
    "            try:\n",
    "                model.load_state_dict(state['model'], strict=False)\n",
    "                print(\"✅ HuBERT model loaded (partial weights)\")\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Partial HuBERT weights loaded with errors: {e}\")\n",
    "                return model\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load HuBERT model: {e}\")\n",
    "        # Fallback to minimal working model\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 768, kernel_size=10, stride=5),\n",
    "            torch.nn.GELU()\n",
    "        )\n",
    "\n",
    "# Initialize VC\n",
    "vc = VC()\n",
    "\n",
    "try:\n",
    "    # Device configuration\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    vc.config = type(\"Config\", (), {\n",
    "        \"is_half\": False,\n",
    "        \"device\": device\n",
    "    })()\n",
    "    print(f\"✅ Device set to: {device}\")\n",
    "\n",
    "    # Load model configuration\n",
    "    model_path = f\"{repo_root}/pretrained_v2/D32k.pth\"\n",
    "    print(f\"🔍 Loading model from: {model_path}\")\n",
    "    try:\n",
    "        cpt = torch.load(model_path, map_location=\"cpu\")\n",
    "        cpt[\"weight\"] = cpt.get(\"model\", {})\n",
    "        vc.cpt = cpt\n",
    "        print(\"✅ Model checkpoint loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model checkpoint: {e}\")\n",
    "        cpt = {\"weight\": {}}\n",
    "        vc.cpt = cpt\n",
    "\n",
    "    # Initialize synthesizer with proper config\n",
    "    print(\"🔄 Initializing synthesizer...\")\n",
    "    try:\n",
    "        vc.net_g = SynthesizerTrnMs256NSFsid(\n",
    "            spec_channels=256,\n",
    "            segment_size=128,\n",
    "            inter_channels=192,\n",
    "            hidden_channels=192,\n",
    "            filter_channels=768,\n",
    "            n_heads=2,\n",
    "            n_layers=6,\n",
    "            kernel_size=3,\n",
    "            p_dropout=0.1,\n",
    "            resblock=ResBlock2,\n",
    "            resblock_kernel_sizes=[3,3,3],\n",
    "            resblock_dilation_sizes=[[1,3,5]]*3,\n",
    "            upsample_rates=[4,4,2],\n",
    "            upsample_initial_channel=256,\n",
    "            upsample_kernel_sizes=[16,16,4],\n",
    "            spk_embed_dim=256,\n",
    "            gin_channels=256,\n",
    "            sr=32000,\n",
    "            is_half=vc.config.is_half\n",
    "        )\n",
    "        vc.net_g.eval().to(device)\n",
    "        print(\"✅ Synthesizer initialized\")\n",
    "        \n",
    "        # Load weights if available\n",
    "        if \"weight\" in cpt:\n",
    "            try:\n",
    "                missing, unexpected = vc.net_g.load_state_dict(cpt[\"weight\"], strict=False)\n",
    "                print(f\"✅ Loaded weights (missing: {len(missing)}, unexpected: {len(unexpected)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to load weights: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to initialize synthesizer: {e}\")\n",
    "        vc.net_g = None\n",
    "\n",
    "    # Load HuBERT model\n",
    "    hubert_path = f\"{repo_root}/pretrained_v2/hubert_base.pt\"\n",
    "    print(f\"🔍 Loading HuBERT from: {hubert_path}\")\n",
    "    vc.hubert_model = load_hubert_model(hubert_path, device)\n",
    "    \n",
    "    if vc.hubert_model is not None:\n",
    "        vc.hubert_model = vc.hubert_model.to(device)\n",
    "        vc.hubert_model.eval()\n",
    "        print(f\"✅ HuBERT model moved to {device}\")\n",
    "    else:\n",
    "        print(\"❌ No HuBERT model available - voice conversion will not work\")\n",
    "\n",
    "    # Initialize pipeline\n",
    "    vc.pipeline = Pipeline(vc.tgt_sr, vc.config)\n",
    "    print(\"✅ Pipeline initialized\")\n",
    "\n",
    "    # ========== AUDIO PROCESSING ==========\n",
    "    input_folder = Path(\"/kaggle/input/eng-audios\")\n",
    "    output_folder = Path(\"/kaggle/working/English_outputs\")\n",
    "    output_folder.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    wav_files = list(input_folder.glob(\"*.wav\")) + list(input_folder.glob(\"*.WAV\"))\n",
    "    \n",
    "    if not wav_files:\n",
    "        print(f\"⚠️ No WAV files found in {input_folder}\")\n",
    "        test_path = input_folder / \"test.wav\"\n",
    "        test_audio = np.sin(2 * np.pi * 440 * np.linspace(0, 1, 16000)).astype(np.float32)\n",
    "        wavfile.write(test_path, 16000, test_audio)\n",
    "        wav_files = [test_path]\n",
    "        print(f\"✅ Created test audio at {test_path}\")\n",
    "\n",
    "    for wav_path in wav_files:\n",
    "        print(f\"\\n🔄 Processing {wav_path.name}\")\n",
    "        try:\n",
    "            # Clear memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Process audio\n",
    "            tgt_sr, audio_opt, _, err = vc.vc_inference(\n",
    "                sid=0,\n",
    "                input_audio_path=str(wav_path),\n",
    "                f0_method=\"rmvpe\",\n",
    "                index_rate=0.75,\n",
    "                protect=0.33\n",
    "            )\n",
    "            \n",
    "            if err is not None or audio_opt is None:\n",
    "                print(f\"❌ Processing failed: {err}\")\n",
    "            else:\n",
    "                # Normalize audio\n",
    "                peak = np.max(np.abs(audio_opt))\n",
    "                if peak > 0:\n",
    "                    audio_opt = (audio_opt / peak) * 0.95\n",
    "                output_path = output_folder / wav_path.name\n",
    "                wavfile.write(output_path, tgt_sr, audio_opt)\n",
    "                print(f\"✅ Saved: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed completely: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Initialization failed: {str(e)}\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"⚠️ Running with limited functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683f55f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define source folder and destination zip file\n",
    "source_folder = '/kaggle/working/English_outputs'\n",
    "zip_file = '/kaggle/working/English_outputs.zip'\n",
    "\n",
    "# Create zip archive\n",
    "shutil.make_archive(zip_file.replace(\".zip\", \"\"), 'zip', source_folder)\n",
    "\n",
    "print(f\"✅ Zipped successfully to: {zip_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7672509,
     "sourceId": 12181864,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672548,
     "sourceId": 12181912,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672619,
     "sourceId": 12182013,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672653,
     "sourceId": 12182058,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672685,
     "sourceId": 12182100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672951,
     "sourceId": 12182447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7672979,
     "sourceId": 12182485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7673071,
     "sourceId": 12182602,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7673163,
     "sourceId": 12182749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7673193,
     "sourceId": 12182786,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7734702,
     "sourceId": 12273871,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.27224,
   "end_time": "2025-06-25T01:12:55.277366",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-25T01:09:29.005126",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
